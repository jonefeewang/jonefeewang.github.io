<!DOCTYPE html><html lang="zh-CN"><head><meta name="google-site-verification" content="IqPxX6jMzoqSIvuvpvuP9PCF7niBjR9EBY0UfP9MHEU"><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content="王军飞的博客"><title>磁盘读写优化技术研究-Zookeeper/Kafka/MySQL | 王军飞的随笔</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=1.0.0"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/normalize/latest/normalize.min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/latest/pure-min.min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/latest/grids-responsive-min.min.css"><link rel="stylesheet" href="//lib.baomitu.com/font-awesome/4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//lib.baomitu.com/jquery/latest/jquery.min.js"></script><link rel="icon" mask="" sizes="any" href="/favicon.ico"><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"><link rel="alternate" type="application/atom+xml" href="/atom.xml"><script type="text/javascript" src="//lib.baomitu.com/clipboard.js/latest/clipboard.min.js"></script><script type="text/javascript" src="//lib.baomitu.com/toastr.js/latest/toastr.min.js"></script><link rel="stylesheet" href="//lib.baomitu.com/toastr.js/latest/toastr.min.css"><meta name="generator" content="Hexo 7.1.1">
<style>.github-emoji { position: relative; display: inline-block; width: 1.2em; min-height: 1.2em; overflow: hidden; vertical-align: top; color: transparent; }  .github-emoji > span { position: relative; z-index: 10; }  .github-emoji img, .github-emoji .fancybox { margin: 0 !important; padding: 0 !important; border: none !important; outline: none !important; text-decoration: none !important; user-select: none !important; cursor: auto !important; }  .github-emoji img { height: 1.2em !important; width: 1.2em !important; position: absolute !important; left: 50% !important; top: 50% !important; transform: translate(-50%, -50%) !important; user-select: none !important; cursor: auto !important; } .github-emoji-fallback { color: inherit; } .github-emoji-fallback img { opacity: 0 !important; }</style>
</head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">磁盘读写优化技术研究-Zookeeper/Kafka/MySQL</h1><a id="logo" href="/.">王军飞的随笔</a><p class="description">always be the best</p></div><div id="nav-menu"><a class="current" href="/."><i class="fa fa-home"> 首页</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/about/"><i class="fa fa-user"> 关于</i></a><a href="/atom.xml"><i class="fa fa-rss"> 订阅</i></a></div></div><div class="pure-g" id="layout"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">磁盘读写优化技术研究-Zookeeper/Kafka/MySQL</h1><div class="post-meta">2020-03-15<span> | </span><span class="category"><a href="/categories/%E7%A3%81%E7%9B%98%E8%AF%BB%E5%86%99%E4%BC%98%E5%8C%96/">磁盘读写优化</a></span><script src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" async></script><span id="busuanzi_container_page_pv"> | <span id="busuanzi_value_page_pv"></span><span> 阅读</span></span><span class="post-time"><span class="post-meta-item-text"> | </span><span class="post-meta-item-icon"><i class="fa fa-keyboard-o"></i><span class="post-count"> 4.5k</span><span class="post-meta-item-text"> 字</span></span></span><span class="post-time"> | <span class="post-meta-item-icon"><i class="fa fa-clock-o"></i><span class="post-count"> 19</span><span class="post-meta-item-text"> 分钟</span></span></span></div><a class="disqus-comment-count" href="/2020/03/15/%E7%A3%81%E7%9B%98%E8%AF%BB%E5%86%99%E4%BC%98%E5%8C%96%E6%8A%80%E6%9C%AF%E7%A0%94%E7%A9%B6-Zookeeper-Kafka-MySQL/#vcomment"><span class="valine-comment-count" data-xid="/2020/03/15/%E7%A3%81%E7%9B%98%E8%AF%BB%E5%86%99%E4%BC%98%E5%8C%96%E6%8A%80%E6%9C%AF%E7%A0%94%E7%A9%B6-Zookeeper-Kafka-MySQL/"></span><span> 条评论</span></a><div class="clear"><div class="toc-article" id="toc"><div class="toc-title">文章目录</div><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%91%98%E8%A6%81"><span class="toc-number">1.</span> <span class="toc-text">摘要</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%85%B3%E9%94%AE%E8%AF%8D"><span class="toc-number">2.</span> <span class="toc-text">关键词</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86"><span class="toc-number">3.</span> <span class="toc-text">基础知识</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F"><span class="toc-number">3.1.</span> <span class="toc-text">1.文件系统</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-%E6%95%B0%E6%8D%AE%E8%90%BD%E7%9B%98"><span class="toc-number">3.2.</span> <span class="toc-text">2. 数据落盘?</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-Zero-Copy%E6%8A%80%E6%9C%AF"><span class="toc-number">3.3.</span> <span class="toc-text">2.Zero Copy技术</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-mmap%E6%8A%80%E6%9C%AF"><span class="toc-number">3.4.</span> <span class="toc-text">3.mmap技术</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-Page-Cache"><span class="toc-number">3.5.</span> <span class="toc-text">4.Page Cache</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#ZooKeeper%E7%A3%81%E7%9B%98%E5%86%99%E4%BC%98%E5%8C%96"><span class="toc-number">4.</span> <span class="toc-text">ZooKeeper磁盘写优化</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-groupCommit"><span class="toc-number">4.1.</span> <span class="toc-text">1.groupCommit</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-%E6%96%87%E4%BB%B6%E9%A2%84%E5%88%86%E9%85%8D"><span class="toc-number">4.2.</span> <span class="toc-text">2.文件预分配</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-%E7%9C%81%E5%8E%BB%E6%9B%B4%E6%96%B0metadata%E4%BF%A1%E6%81%AF"><span class="toc-number">4.3.</span> <span class="toc-text">3.省去更新metadata信息</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Mysql%E7%9A%84%E7%A3%81%E7%9B%98%E5%86%99%E4%BC%98%E5%8C%96"><span class="toc-number">5.</span> <span class="toc-text">Mysql的磁盘写优化</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-group-commit"><span class="toc-number">5.1.</span> <span class="toc-text">1.group commit</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Kafka%E7%9A%84%E7%A3%81%E7%9B%98%E8%AF%BB%E5%86%99%E4%BC%98%E5%8C%96"><span class="toc-number">6.</span> <span class="toc-text">Kafka的磁盘读写优化</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-page-cache%E7%9A%84%E4%BD%BF%E7%94%A8"><span class="toc-number">6.1.</span> <span class="toc-text">1.page cache的使用</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-zeroCopy%E6%8A%80%E6%9C%AF%E7%9A%84%E4%BD%BF%E7%94%A8"><span class="toc-number">6.2.</span> <span class="toc-text">2. zeroCopy技术的使用</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-%E6%96%87%E4%BB%B6%E9%A2%84%E5%88%86%E9%85%8D"><span class="toc-number">6.3.</span> <span class="toc-text">3.文件预分配</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-mmap%E6%8A%80%E6%9C%AF"><span class="toc-number">6.4.</span> <span class="toc-text">4. mmap技术</span></a></li></ol></li></ol></div></div><div class="post-content"><p><span class="github-emoji" style="display:inline;vertical-align:middle"><span>⚠</span><img src="https://github.githubassets.com/images/icons/emoji/unicode/26a0.png?v8" aria-hidden="true" onerror="this.parent.classList.add('github-emoji-fallback')"></span> 原创文章，转载请注明出处</p>
<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a><a href="#%E6%91%98%E8%A6%81" title="摘要"></a>摘要</h2><p>本文对linux下磁盘的读写优化技术做了一个搜底和总览，包括基本的文件读写知识，磁盘，page cache ，zeror copy，mmap等知识，还对zookeeper 、kafka 、mysql 在使用磁盘读写文件时的优化技术做了分析。</p>
<h2 id="关键词"><a href="#关键词" class="headerlink" title="关键词"></a><a href="#%E5%85%B3%E9%94%AE%E8%AF%8D" title="关键词"></a>关键词</h2><p>磁盘读写, 文件系统, block, page cache, zero copy, mmap, zookeeper, kafka, mysql</p>
<h2 id="基础知识"><a href="#基础知识" class="headerlink" title="基础知识"></a><a href="#%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86" title="基础知识"></a>基础知识</h2><h3 id="1-文件系统"><a href="#1-文件系统" class="headerlink" title="1.文件系统"></a><a href="#1-%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F" title="1.文件系统"></a>1.文件系统</h3><p>Linux使用文件系统来管理磁盘，将此磁盘分为一个个“block“单元，如下图所示，文件内容被写进一段连续的block单元内，中间空出来的即是我们平常所说的“碎片空间(fragment)”。常见的文件系统包含ext/ext2/ext3/ext4，以及windows使用NTFS等。</p>
<p>在现代ext4文件系统上，碎片空间已经不是问题，因为在分配每个文件存储的位置时，系统会特意让两个文件之间距离较远的位置，这样既方便文件不断变大，也不需要去整理碎片，但是极端场景下，比如整块磁盘空间都快用完的时候，就需要去挪动或迁移碎片，腾出可用空间，正常情况下是不需要的。</p>
<p><img src="/2020/03/15/%E7%A3%81%E7%9B%98%E8%AF%BB%E5%86%99%E4%BC%98%E5%8C%96%E6%8A%80%E6%9C%AF%E7%A0%94%E7%A9%B6-Zookeeper-Kafka-MySQL/file_blocks.svg" title="file blocks"></p>
<p>block内除了存储文件的真实内容之外，还需要存储每个文件的属性信息，<br>比如gid/uid/permission/atime/modtime，以及文件存储在哪些block上的信息等元数据， 这类数据称之为metadata，在linux操作系统中叫index node，简称inode。ext3之前的文件系统，indoe需要使用一个集合来存储某个文件写在了哪些block上，随着文件的增大，这个集合会越来越大，造成系统的一个瓶颈。ext3之后的inode设计上，不再记录block集合信息，而是采用记录开始位置，和文件大小的两个属性，如下所示 ：</p>
<figure class="highlight abnf"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">struct ext3_extent {  </span><br><span class="line"> __le32  ee_block<span class="comment">; /\* first logical block extent covers 文件的开始block信息 \*/   </span></span><br><span class="line"> __le16  ee_len<span class="comment">;   /\* number of blocks covered by extent 文件的大小\*/  </span></span><br><span class="line"> __le16  ee_start_hi<span class="comment">;  /\* high 16 bits of physical block \*/  </span></span><br><span class="line"> __le32 ee_start<span class="comment">; /\* low 32 bits of physical block \*/  </span></span><br><span class="line">}<span class="comment">;  </span></span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure>
<h3 id="2-数据落盘"><a href="#2-数据落盘" class="headerlink" title="2. 数据落盘?"></a><a href="#2-%E6%95%B0%E6%8D%AE%E8%90%BD%E7%9B%98" title="2. 数据落盘?"></a>2. 数据落盘?</h3><p>由上边可以看出，读、写文件时，除了写文件内容本身之外，还要至少一次磁盘寻址，来写metadata一类的信息，总结这些额外的信息包括下边几个:</p>
<ol>
<li>文件所占的block数量</li>
<li>文件最后一次访问时间atime</li>
<li>文件最后一次的修改时间</li>
</ol>
<p>针对这些额外的inode信息写操作，linux写文件时也有相应的两个函数来处理:</p>
<ol>
<li>fsync() : 将文件内容+文件metadata数据同步刷到磁盘上</li>
<li>fdatasync(): 只将文件内容同步刷到磁盘上</li>
</ol>
<p>java中的FileChannel api也提供了这个选项</p>
<figure class="highlight java"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> @param metdata 是否同时将metadata信息同步flush到磁盘上  </span></span><br><span class="line"><span class="comment">*/</span>  </span><br><span class="line"><span class="keyword">public</span> <span class="keyword">abstract</span> <span class="keyword">void</span> <span class="title function_">force</span><span class="params">(<span class="type">boolean</span> metaData)</span> <span class="keyword">throws</span> IOException  </span><br></pre></td></tr></tbody></table></figure>
<p>因此，在高频次写文件操作时，可以使用fdatasync减少一次磁盘操作，来提升写速度。</p>
<p>另外两个比较混淆的文件操作:</p>
<ol>
<li>FileDescriptor.sync() 底层同样调用的是fsync，将文件内容+文件metadata信息全部刷到了磁盘上</li>
<li>OutputStream.flush只是将缓冲区内的数据发给操作系统，但有可能并未落盘，可能在pagecache内。</li>
</ol>
<p>注意:<br>OutputStream和FileoutputStream是一空函数,但 FilterOutputStream.flush<br>,BufferedOutputStream.flush,ObjectOutputStream.flush确实是有内容的。</p>
<p>如果要把文件内容真正落盘，需要先调用stream的flush，将数据发送给操作系统，然后在调用FileDiscriptor的sync方法或 FileChannel.force方法，将数据真正落盘.</p>
<h3 id="2-Zero-Copy技术"><a href="#2-Zero-Copy技术" class="headerlink" title="2.Zero Copy技术"></a><a href="#2-Zero-Copy%E6%8A%80%E6%9C%AF" title="2.Zero Copy技术"></a>2.Zero Copy技术</h3><p>在Java程序内通过InputStream和OutputStream读取或写入文件数据时，操作系统底层的实现一般是这样的：  </p>
<p><img src="/2020/03/15/%E7%A3%81%E7%9B%98%E8%AF%BB%E5%86%99%E4%BC%98%E5%8C%96%E6%8A%80%E6%9C%AF%E7%A0%94%E7%A9%B6-Zookeeper-Kafka-MySQL/read_write_file.svg" title="read and write a file"></p>
<ol>
<li>在用户空间内，程序发出读取数据指令。</li>
<li>操作系统切换至内核空间，内核通过DMA读取磁盘数据。</li>
<li>内核将自己读到的数据，从自己的buffer copy到用户的buffer内</li>
<li>操作系统切换至用户空间</li>
<li>继续用户处理逻辑</li>
<li>完成处理后，将数据写入磁盘时，需要将buffer copy到内核的buffer内。</li>
<li>操作系统切换至内核空间</li>
<li>内核将数据通过disk controller写入磁盘</li>
<li>操作系统切换至用户空间继续</li>
</ol>
<p>可以看到，一个简单文件处理后保存操作，涉及到4次上下文切换和两次copy.</p>
<p>但linux系统都提供一种zero copy的技术支持，在用户空间内这个函数通常叫sendFile，可以使用linux man 命令(man sendfile)查看到，他的工作原理如下:</p>
<p><img src="/2020/03/15/%E7%A3%81%E7%9B%98%E8%AF%BB%E5%86%99%E4%BC%98%E5%8C%96%E6%8A%80%E6%9C%AF%E7%A0%94%E7%A9%B6-Zookeeper-Kafka-MySQL/zero_copy.svg" title="zero copy"></p>
<p>上图可见，用在程序里调用sendfile后，操作系统将数据从磁盘读入buffer后，直接发送给了网络，没有再把buffer拷贝到用户空间，但是仍然会有两次上下文切换。</p>
<p>nginx和apache httpd 服务器都支持sendfile指令。</p>
<p>具体到java中的“sendfile” 是FileChannel的transferto方法</p>
<figure class="highlight aspectj"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">abstract</span> <span class="keyword">long</span> transferTo(<span class="keyword">long</span> position,  </span><br><span class="line"> <span class="keyword">long</span> count,  </span><br><span class="line"> WritableByteChannel <span class="keyword">target</span>)  </span><br><span class="line"> <span class="keyword">throws</span> IOException  </span><br><span class="line"><span class="comment">// Transfers bytes from this channel's file to the given writable byte channel.  </span></span><br></pre></td></tr></tbody></table></figure>
<h3 id="3-mmap技术"><a href="#3-mmap技术" class="headerlink" title="3.mmap技术"></a><a href="#3-mmap%E6%8A%80%E6%9C%AF" title="3.mmap技术"></a>3.mmap技术</h3><p>上边所说的sendfile使用zero copy技术，实际上我们并没有修改文件内容， 然后再保存，只是简单的将本次磁盘上的文件内容发送给了网络。</p>
<p>现代操作系统都支持一种技术叫mmap，将文件内容直接映射到用户空间内的一块内存buffer上，可以读也可以写，省去了kernel到用户空间的多次copy。</p>
<p><img src="/2020/03/15/%E7%A3%81%E7%9B%98%E8%AF%BB%E5%86%99%E4%BC%98%E5%8C%96%E6%8A%80%E6%9C%AF%E7%A0%94%E7%A9%B6-Zookeeper-Kafka-MySQL/mmap.svg" title="mmap"></p>
<p>这种技术能在内存中修改文件的内容，修改后还能保存，省去了buffer 在kernel和用户之间的copy，所以速度非常快。它实际上使用的类似OS管理虚拟内存的方式，将文件内容page in /page out。对应到java里的api 就是MappedByteBuffer，实际上也是一个byte buffer，同 directBuffer一样，这些内存都不在jvm的堆内。</p>
<p>使用MappedByteBuffer时，直接指定开始读的文件位置，和需要读多少长度的内容。</p>
<figure class="highlight sas"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">MappedByteBuffer <span class="keyword">out</span> = <span class="keyword">file</span>.getChannel().map(FileChannel.MapMode.READ_WRITE, 0, <span class="keyword">length</span>);  </span><br></pre></td></tr></tbody></table></figure>
<p>但这种技术也是有缺点的：</p>
<ol>
<li>每个mmap生成的文件handle在一个进程内是有限制的，在linux下是64k，如果超过整个数字，就无法在map文件</li>
<li>由于各种原因，jvm没有暴露关闭文件映射的方法，不能显式调用munmap()，要关闭掉这个文件映射，只能将MappedByteBuffer置为null，等java的gc过来回收，但如果内存很大，迟迟未gc的话，会耗尽文件handle。另外因为映射未及时关闭的话，不能再用普通的文件IO操作来读写文件。要显式关闭的话，只能使用hack的方法来调用sun.misc.Cleaner。</li>
<li>对文件的所有修改都是在内存中，可以调用MappedByteBuffer的force方法，强制将内容flush到磁盘上，但是如果每次都这样强刷，是有性能损耗的。但如果让操作系统来管理什时候flush的话，机器突然宕机会有丢失数据的风险</li>
<li>每次mmap文件时只能产生最大2GB的空间。</li>
</ol>
<h3 id="4-Page-Cache"><a href="#4-Page-Cache" class="headerlink" title="4.Page Cache"></a><a href="#4-Page-Cache" title="4.Page Cache"></a>4.Page Cache</h3><p>在上边第二部分已经说过，数据不落盘的话，就是在操作系统的缓冲区内，这里就是pageCache，Linux操作系统默认在文件写入时，为了加快写入的速度，都会先写入page cache。page cache 实际是内存的缓冲区，在内存的分页Page内，写入数据的page称之为脏页(dirtyPage)，需要随后flush到磁盘上。</p>
<p>在读取文件数据时，操作系统读完之后，也会将数据暂存在page cache上，第二次读相同文件时，数据已经在内存page cache 内了，所以会更快一些。</p>
<p>Linux操作系统会将所有可用的内存作为page cache使用，物尽其用，不让其浪费，这就是在linux下看free memory时总是非常小的原因。</p>
<p>在数据写入page cache后，操作系统负载flush数据的线程会监视两个值，第一个是dirtypage 里的数据最长能存活多长时间必须刷入磁盘，第二个是flush线程多久需要 wakeup起来运行一次。flush线程运行时，除了看脏页存活时间外，还会看另外两个值，dirty_background_ratio和dirty_ratio.</p>
<p>这两个参数的意义：</p>
<ol>
<li>vm.dirty_background_ratio 是指dirtyPage占总内存百分比多少的时候，系统开始强制将dirtyPage的数据刷入磁盘。</li>
<li>vm.dirty_ratio 指dirtyPage占据多少百分比内存时，开始强制block所有的IO操作，强制将dirtyPage内的数据刷入磁盘。</li>
</ol>
<p>简单说，就是第一个控制dirtyPage可以有多大，但有可能第一个条件达到时， flush线程还未到达触发条件，第二控制是兜底，最大不能超过多大，否则所有的IO都暂停，强刷数据到磁盘上。</p>
<p>pageCache不能设置太大，也不能设置太小。太大的话，脏页里的数据会太多，刷入磁盘时卡顿时间会很长，太小的话，又起不到写数据是加速的作用。</p>
<h2 id="ZooKeeper磁盘写优化"><a href="#ZooKeeper磁盘写优化" class="headerlink" title="ZooKeeper磁盘写优化"></a><a href="#ZooKeeper%E7%A3%81%E7%9B%98%E5%86%99%E4%BC%98%E5%8C%96" title="ZooKeeper磁盘写优化"></a>ZooKeeper磁盘写优化</h2><h3 id="1-groupCommit"><a href="#1-groupCommit" class="headerlink" title="1.groupCommit"></a><a href="#1-groupCommit" title="1.groupCommit"></a>1.groupCommit</h3><p>在“一致性协议研究-zookeeper”章节中说过，zookeeper运行过程中，会生成两个文件，一个是txn-log，一个是snapShot文件。txn-log是事务日志，每个写请求都会转发给leader，leader发送propose数据给follower，flollower将数据落盘，然后ack 集群leader。Leader在收到足够的ack数量后，会发送commit请求给 follower，正式完成写请求。</p>
<p>如上一章所属，为了保证一致性，follower必须将数据“真正”落盘后，才能ack集群leader。如果在流量很大的情况下，这种频繁落盘操作势必会成为瓶颈，zookeeper怎么来解决这个性能问题呢？</p>
<p>follower在接收到leader的propose请求后，准确的说并未真实落盘，仍然在缓冲区内，zookeeper会缓存多条事务，等设定的阈值条数到了之后，再写入磁盘。相关代码如下:</p>
<figure class="highlight java"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 插入新的事务，将事务序列化到文件中  </span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">synchronized</span> <span class="type">boolean</span> <span class="title function_">append</span><span class="params">(TxnHeader hdr, Record txn)</span>  </span><br><span class="line"> <span class="keyword">throws</span> IOException{  </span><br><span class="line"> <span class="comment">//..  </span></span><br><span class="line"> fos = <span class="keyword">new</span> <span class="title class_">FileOutputStream</span>(logFileWrite);  </span><br><span class="line"> logStream=<span class="keyword">new</span> <span class="title class_">BufferedOutputStream</span>(fos);  </span><br><span class="line"> oa = BinaryOutputArchive.getArchive(logStream);  </span><br><span class="line"> <span class="type">FileHeader</span> <span class="variable">fhdr</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">FileHeader</span>(TXNLOG_MAGIC,VERSION, dbId);  </span><br><span class="line"> fhdr.serialize(oa, <span class="string">"fileheader"</span>);  </span><br><span class="line"> <span class="comment">//...  </span></span><br><span class="line">}  </span><br><span class="line">  </span><br><span class="line"><span class="comment">//继续，插入新的事务  </span></span><br><span class="line"> toFlush.add(si);  </span><br><span class="line"> <span class="keyword">if</span> (toFlush.size() &gt; <span class="number">1000</span>) {  </span><br><span class="line"> <span class="comment">//积累到1000条后，开始flush，真正落盘  </span></span><br><span class="line"> flush(toFlush);  </span><br><span class="line"> }  </span><br><span class="line">  </span><br><span class="line"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title function_">flush</span><span class="params">(LinkedList&lt;Request&gt; toFlush)</span>  </span><br><span class="line"> <span class="keyword">throws</span> IOException, RequestProcessorException  </span><br><span class="line"> {  </span><br><span class="line"> <span class="comment">//...  </span></span><br><span class="line"> <span class="comment">// 调用 zk database 的commit ，真正落盘开始  </span></span><br><span class="line"> zks.getZKDatabase().commit();  </span><br><span class="line"> <span class="comment">//..  </span></span><br><span class="line"> }  </span><br><span class="line">   </span><br><span class="line"><span class="keyword">public</span> <span class="keyword">synchronized</span> <span class="keyword">void</span> <span class="title function_">commit</span><span class="params">()</span> <span class="keyword">throws</span> IOException {  </span><br><span class="line"> <span class="comment">//...  </span></span><br><span class="line"> <span class="keyword">for</span>(FileOutputStream log : streamsToFlush)  </span><br><span class="line"> log.getChannel().force(<span class="literal">false</span>); <span class="comment">//落盘  </span></span><br><span class="line"> <span class="comment">///...  </span></span><br><span class="line"> }  </span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure>
<p>以上代码摘自SyncRequestProcessor和FilTxnLog两个类。有代码可见，group commit其实就是batch commit.</p>
<h3 id="2-文件预分配"><a href="#2-文件预分配" class="headerlink" title="2.文件预分配"></a><a href="#2-%E6%96%87%E4%BB%B6%E9%A2%84%E5%88%86%E9%85%8D" title="2.文件预分配"></a>2.文件预分配</h3><p>linux系统下，文件的一次写入最少会包含两次磁盘寻址，一次是写文件的真正内容，另外一次是更新文件的metadata信息，如上文所说的inode信息，需要更新使用的block数量，文件修改时间等信息。 zookeeper中的事务日志文件写入，采用了预分配的方法，每次创建一个新文件时，预先分配一定数量的文件空间(默认64M)(预定数量的block)，这样在平时高速追加文件内容时，就不需要每次去更新文件的meta信息，增加一次磁盘寻址了。相关的代码如下:</p>
<figure class="highlight arduino"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment">/*  </span></span><br><span class="line"><span class="comment"> \* Grows the file to the specified number of bytes. This only happenes if   </span></span><br><span class="line"><span class="comment"> \* the current file position is sufficiently close (less than 4K) to end of   </span></span><br><span class="line"><span class="comment"> \* file.   </span></span><br><span class="line"><span class="comment"> \*   </span></span><br><span class="line"><span class="comment"> \* @param f output stream to pad  </span></span><br><span class="line"><span class="comment"> \* @param currentSize application keeps track of the cuurent file size  </span></span><br><span class="line"><span class="comment"> \* @param preAllocSize how many bytes to pad  </span></span><br><span class="line"><span class="comment"> \* @return the new file size. It can be the same as currentSize if no  </span></span><br><span class="line"><span class="comment"> \* padding was done.  </span></span><br><span class="line"><span class="comment"> \* @throws IOException  </span></span><br><span class="line"><span class="comment"> */</span>  </span><br><span class="line"> <span class="function"><span class="keyword">public</span> <span class="type">static</span> <span class="type">long</span> <span class="title">padLogFile</span><span class="params">(FileOutputStream f,<span class="type">long</span> currentSize,  </span></span></span><br><span class="line"><span class="params"><span class="function"> <span class="type">long</span> preAllocSize)</span> throws IOException</span>{  </span><br><span class="line"> <span class="type">long</span> position = f.<span class="built_in">getChannel</span>().<span class="built_in">position</span>();  </span><br><span class="line"> <span class="comment">//判断当前的文件空间是否马上(差4kb)就要写满  </span></span><br><span class="line"> <span class="keyword">if</span> (position + <span class="number">4096</span> &gt;= currentSize) {  </span><br><span class="line"> currentSize = currentSize + preAllocSize;  </span><br><span class="line"> fill.<span class="built_in">position</span>(<span class="number">0</span>);  </span><br><span class="line"> <span class="comment">//预分配一个新的64M空间的文件  </span></span><br><span class="line"> f.<span class="built_in">getChannel</span>().<span class="built_in">write</span>(fill, currentSize-fill.<span class="built_in">remaining</span>());  </span><br><span class="line"> }  </span><br><span class="line"> <span class="keyword">return</span> currentSize;  </span><br><span class="line"> }  </span><br></pre></td></tr></tbody></table></figure>
<h3 id="3-省去更新metadata信息"><a href="#3-省去更新metadata信息" class="headerlink" title="3.省去更新metadata信息"></a><a href="#3-%E7%9C%81%E5%8E%BB%E6%9B%B4%E6%96%B0metadata%E4%BF%A1%E6%81%AF" title="3.省去更新metadata信息"></a>3.省去更新metadata信息</h3><figure class="highlight processing"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment">/*  </span></span><br><span class="line"><span class="comment"> * commit the logs. make sure that evertyhing hits the  </span></span><br><span class="line"><span class="comment"> * disk  </span></span><br><span class="line"><span class="comment"> */</span>  </span><br><span class="line"> <span class="keyword">public</span> <span class="keyword">synchronized</span> <span class="keyword">void</span> <span class="title function_">commit</span>() <span class="keyword">throws</span> IOException {  </span><br><span class="line"> <span class="keyword">if</span> (logStream != <span class="literal">null</span>) {  </span><br><span class="line"> logStream.<span class="property">flush</span>();  </span><br><span class="line"> }  </span><br><span class="line"> <span class="keyword">for</span> (FileOutputStream <span class="built_in">log</span> : streamsToFlush) {  </span><br><span class="line"> <span class="built_in">log</span>.<span class="property">flush</span>();  </span><br><span class="line"> <span class="keyword">if</span> (forceSync) {  </span><br><span class="line"> <span class="type">long</span> startSyncNS = System.<span class="property">nanoTime</span>();  </span><br><span class="line"> <span class="comment">//FileChannel.force直写数据，不更新metadata，少一次磁盘寻道操作，优化写速度  </span></span><br><span class="line"> <span class="built_in">log</span>.<span class="property">getChannel</span>().<span class="property">force</span>(<span class="literal">false</span>);  </span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure>
<h2 id="Mysql的磁盘写优化"><a href="#Mysql的磁盘写优化" class="headerlink" title="Mysql的磁盘写优化"></a><a href="#Mysql%E7%9A%84%E7%A3%81%E7%9B%98%E5%86%99%E4%BC%98%E5%8C%96" title="Mysql的磁盘写优化"></a>Mysql的磁盘写优化</h2><h3 id="1-group-commit"><a href="#1-group-commit" class="headerlink" title="1.group commit"></a><a href="#1-group-commit" title="1.group commit"></a>1.group commit</h3><p>mysql 中binlog的写入同样是每条事务的落盘，多个mysql 线程都同时在处理事务，怎么保障写binlog时不会冲突、乱序呢。这里边就需要一个锁(queue锁)，拿到锁的线程将事务写进一个统一的落盘事务queue，写入后释放锁，其他的thread可以继续写入新的事务。如下代码所示:</p>
<p>代码摘自:<br><a target="_blank" rel="noopener" href="https://github.com/percona/percona-server/blob/1b5dff5b9e5f8c797cfed966c73fbbf6d45cbd59/sql/log.cc">https://github.com/percona/percona-server/blob/1b5dff5b9e5f8c797cfed966c73fbbf6d45cbd59/sql/log.cc</a></p>
<figure class="highlight xl"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"> <span class="comment">//拿到queue锁  </span></span><br><span class="line">mysql_mutex_lock(&amp;LOCK_group_commit_queue);  </span><br><span class="line">group_commit_entry \*orig_queue= group_commit_queue;  </span><br><span class="line"><span class="function"><span class="title">entry</span>-&gt;</span>next= orig_queue;  </span><br><span class="line"> <span class="comment">//追加本次事务  </span></span><br><span class="line">group_commit_queue= entry;  </span><br><span class="line">DEBUG_SYNC(<span class="function"><span class="title">entry</span>-&gt;</span>thd, <span class="string">"commit_group_commit_queue"</span>);  </span><br><span class="line"><span class="comment">//释放锁  </span></span><br><span class="line">mysql_mutex_unlock(&amp;LOCK_group_commit_queue);  </span><br></pre></td></tr></tbody></table></figure>
<p>如果这条线程发现自己是第一个初始化quque的，那么他自动变为group commit leader线程，他要负责将queue的内容最终落盘到binlog文件里去。</p>
<figure class="highlight scss"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment">/*  </span></span><br><span class="line"><span class="comment"> The first in the queue handle group commit for all; the others just wait  </span></span><br><span class="line"><span class="comment"> to be signalled when group commit is done.  </span></span><br><span class="line"><span class="comment"> */</span>  </span><br><span class="line"> if (orig_queue != NULL)  </span><br><span class="line"> entry-&gt;thd-&gt;<span class="built_in">wait_for_wakeup_ready</span>();  </span><br><span class="line"> else <span class="comment">//我是group commit leader  </span></span><br><span class="line"> <span class="built_in">trx_group_commit_leader</span>(entry);  </span><br></pre></td></tr></tbody></table></figure>
<p>leader线程会将queue里的内容先拷贝到自己的thread local queue里，然后再将自己thread local queue里的内容写进磁盘</p>
<p>落盘到binlog文件里的操作:</p>
<figure class="highlight dts"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment">/*  </span></span><br><span class="line"><span class="comment"> Lock the LOCK_log(), and once we get it, collect any additional writes  </span></span><br><span class="line"><span class="comment"> that queued up while we were waiting.  </span></span><br><span class="line"><span class="comment"> */</span>  </span><br><span class="line"> <span class="comment">//获取bin log文件锁  </span></span><br><span class="line"> mysql_mutex_lock(<span class="variable">&amp;</span>LOCK_log)<span class="punctuation">;</span>  </span><br><span class="line"> DEBUG_SYNC(leader-&gt;thd, <span class="string">"commit_after_get_LOCK_log"</span>)<span class="punctuation">;</span>  </span><br><span class="line"> <span class="comment">//再次获取queue锁  </span></span><br><span class="line"> mysql_mutex_lock(<span class="variable">&amp;</span>LOCK_group_commit_queue)<span class="punctuation">;</span>  </span><br><span class="line"> <span class="comment">//copy queue到thread local 里的queue里  </span></span><br><span class="line"> <span class="attr">current</span><span class="operator">=</span> group_commit_<span class="attr">queue</span><span class="punctuation">;</span>  </span><br><span class="line"> <span class="comment">//清空原来的queue  </span></span><br><span class="line"> group_commit_<span class="attr">queue</span><span class="operator">=</span> NULL<span class="punctuation">;</span>  </span><br><span class="line"> <span class="comment">//释放queue锁  </span></span><br><span class="line"> mysql_mutex_unlock(<span class="variable">&amp;</span>LOCK_group_commit_queue)<span class="punctuation">;</span>  </span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure>
<p>由上可以看出，mysql的事务写入也是批量的，通过中间的queue(mysql group commit queue)来实现批量的积攒，最终一次落盘操作。</p>
<h2 id="Kafka的磁盘读写优化"><a href="#Kafka的磁盘读写优化" class="headerlink" title="Kafka的磁盘读写优化"></a><a href="#Kafka%E7%9A%84%E7%A3%81%E7%9B%98%E8%AF%BB%E5%86%99%E4%BC%98%E5%8C%96" title="Kafka的磁盘读写优化"></a>Kafka的磁盘读写优化</h2><h3 id="1-page-cache的使用"><a href="#1-page-cache的使用" class="headerlink" title="1.page cache的使用"></a><a href="#1-page-cache%E7%9A%84%E4%BD%BF%E7%94%A8" title="1.page cache的使用"></a>1.page cache的使用</h3><p>Kafka主要用磁盘来存储消息，他对磁盘的使用优化技术用的很多，第一个就是pageCache的使用，kafka默认是不强刷磁盘的，所有的消息全部写入pageCache内，让操作系统来管理刷盘策略。但是kafka仍然提供了两个控制参数，多长时间需要刷一次盘，收到多少条消息后需要刷一次盘，如下代码：</p>
<figure class="highlight arduino"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"> <span class="keyword">public</span> <span class="type">static</span> <span class="keyword">final</span> <span class="type">String</span> FLUSH_MESSAGES_INTERVAL_CONFIG = <span class="string">"flush.messages"</span>;  </span><br><span class="line"> <span class="comment">//注释中可以看出，不建议设置，让操作系统使用后台线程flush  </span></span><br><span class="line"> <span class="keyword">public</span> <span class="type">static</span> <span class="keyword">final</span> <span class="type">String</span> FLUSH_MESSAGES_INTERVAL_DOC = <span class="string">"This setting allows specifying an interval at "</span> +  </span><br><span class="line"> <span class="string">"which we will force an fsync of data written to the log. For example if this was set to 1 "</span> +  </span><br><span class="line"> <span class="string">"we would fsync after every message; if it were 5 we would fsync after every five messages. "</span> +  </span><br><span class="line"> <span class="string">"In general we recommend you not set this and use replication for durability and allow the "</span> +  </span><br><span class="line"> <span class="string">"operating system's background flush capabilities as it is more efficient. This setting can "</span> +  </span><br><span class="line"> <span class="string">"be overridden on a per-topic basis (see &lt;a href=\\"</span><span class="meta">#topicconfigs\\<span class="string">"&gt;the per-topic configuration section&lt;/a&gt;)."</span>;  </span></span><br><span class="line">  </span><br><span class="line"> <span class="keyword">public</span> <span class="type">static</span> <span class="keyword">final</span> <span class="type">String</span> FLUSH_MS_CONFIG = <span class="string">"flush.ms"</span>;  </span><br><span class="line"> <span class="comment">//注释中可以看出，不建议设置，让操作系统使用后台线程flush  </span></span><br><span class="line"> <span class="keyword">public</span> <span class="type">static</span> <span class="keyword">final</span> <span class="type">String</span> FLUSH_MS_DOC = <span class="string">"This setting allows specifying a time interval at which we will "</span> +  </span><br><span class="line"> <span class="string">"force an fsync of data written to the log. For example if this was set to 1000 "</span> +  </span><br><span class="line"> <span class="string">"we would fsync after 1000 ms had passed. In general we recommend you not set "</span> +  </span><br><span class="line"> <span class="string">"this and use replication for durability and allow the operating system's background "</span> +  </span><br><span class="line"> <span class="string">"flush capabilities as it is more efficient."</span>;  </span><br><span class="line">   </span><br><span class="line">  </span><br><span class="line"> <span class="comment">//到达配置时间后，强制刷盘   </span></span><br><span class="line"><span class="keyword">if</span> (unflushedMessages &gt;= config.flushInterval)  </span><br><span class="line"> <span class="built_in">flush</span>()  </span><br><span class="line"> <span class="comment">//xxxx  </span></span><br><span class="line">  </span><br><span class="line"> <span class="comment">/*  </span></span><br><span class="line"><span class="comment"> * Commit all written data to the physical disk  </span></span><br><span class="line"><span class="comment"> */</span>  </span><br><span class="line"> <span class="function"><span class="keyword">public</span> <span class="type">void</span> <span class="title">flush</span><span class="params">()</span> throws IOException </span>{  </span><br><span class="line"> <span class="comment">//强制刷盘，包括元数据也刷进磁盘  </span></span><br><span class="line"> channel.force(<span class="literal">true</span>);  </span><br><span class="line"> }  </span><br></pre></td></tr></tbody></table></figure>

<h3 id="2-zeroCopy技术的使用"><a href="#2-zeroCopy技术的使用" class="headerlink" title="2. zeroCopy技术的使用"></a><a href="#2-zeroCopy%E6%8A%80%E6%9C%AF%E7%9A%84%E4%BD%BF%E7%94%A8" title="2. zeroCopy技术的使用"></a>2. zeroCopy技术的使用</h3><p>kafka在消费者拉取消息时，需要将磁盘的数据发给消费者，这时就是用”sendFile”的zero copy概念，相关代码如下(截取自PlaintextTransportLayer.java):</p>
<figure class="highlight aspectj"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="meta">@Override</span>  </span><br><span class="line"><span class="keyword">public</span> <span class="function"><span class="keyword">long</span> <span class="title">transferFrom</span><span class="params">(FileChannel fileChannel, <span class="keyword">long</span> position, <span class="keyword">long</span> count)</span> <span class="keyword">throws</span> IOException </span>{  </span><br><span class="line"> <span class="comment">//将文件内容写入网络socket，zero copy  </span></span><br><span class="line"> <span class="function"><span class="keyword">return</span> fileChannel.<span class="title">transferTo</span><span class="params">(position, count, socketChannel)</span></span>;  </span><br><span class="line">}  </span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure>
<h3 id="3-文件预分配"><a href="#3-文件预分配" class="headerlink" title="3.文件预分配"></a><a href="#3-%E6%96%87%E4%BB%B6%E9%A2%84%E5%88%86%E9%85%8D" title="3.文件预分配"></a>3.文件预分配</h3><p>(详细内容，参见上面zookeeper-文件预分配技术)，kafka记录消息的文件称之为segement，在创建这样的文件时也是用了 文件预分配技术优化，相关代码如下(截取自Log.scala):</p>
<figure class="highlight stylus"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">  </span><br><span class="line"><span class="keyword">if</span> (logSegments.isEmpty) {  </span><br><span class="line"> <span class="comment">// no existing segments, create a new mutable segment beginning at logStartOffset  </span></span><br><span class="line"> <span class="built_in">addSegment</span>(LogSegment<span class="selector-class">.open</span>(dir = dir,  </span><br><span class="line"> baseOffset = logStartOffset,  </span><br><span class="line"> config,  </span><br><span class="line"> <span class="selector-tag">time</span> = <span class="selector-tag">time</span>,  </span><br><span class="line"> fileAlreadyExists = false,  </span><br><span class="line"> initFileSize = this<span class="selector-class">.initFileSize</span>,  </span><br><span class="line"> <span class="comment">//预分配文件大小  </span></span><br><span class="line"> preallocate = config.preallocate))  </span><br><span class="line">}  </span><br></pre></td></tr></tbody></table></figure>

<h3 id="4-mmap技术"><a href="#4-mmap技术" class="headerlink" title="4. mmap技术"></a><a href="#4-mmap%E6%8A%80%E6%9C%AF" title="4. mmap技术"></a>4. mmap技术</h3><p>内存映射文件，如上边基础知识所述，通过内存来映射磁盘上的数据文件，达到较快的读写速度。kafka通过mmap来读写index文件，相关代码如下(截取自AbstractIndex.scala):</p>
<figure class="highlight scala"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">AbstractIndex\</span>[<span class="type">K</span>, <span class="type">V</span>\](<span class="params">@volatile var file: <span class="type">File</span>, val baseOffset: <span class="type">Long</span>,  </span></span></span><br><span class="line"><span class="params"><span class="class"> val maxIndexSize: <span class="type">Int</span> = -1, val writable: <span class="type">Boolean</span></span>) <span class="keyword">extends</span> <span class="title">Closeable</span> <span class="keyword">with</span> <span class="title">Logging</span> </span>{  </span><br><span class="line">​  </span><br><span class="line"> <span class="comment">// Length of the index file  </span></span><br><span class="line"> <span class="meta">@volatile</span>  </span><br><span class="line"> <span class="keyword">private</span> <span class="keyword">var</span> _length: <span class="type">Long</span> = _  </span><br><span class="line"> <span class="keyword">protected</span> <span class="function"><span class="keyword">def</span> <span class="title">entrySize</span></span>: <span class="type">Int</span>  </span><br><span class="line">​  </span><br><span class="line"> <span class="comment">/*  </span></span><br><span class="line"><span class="comment"> Kafka mmaps index files into memory, and all the read / write operations of the index is through OS page cache. This  </span></span><br><span class="line"><span class="comment"> avoids blocked disk I/O in most cases.  </span></span><br><span class="line"><span class="comment">​  </span></span><br><span class="line"><span class="comment"> To the extent of our knowledge, all the modern operating systems use LRU policy or its variants to manage page  </span></span><br><span class="line"><span class="comment"> cache. Kafka always appends to the end of the index file, and almost all the index lookups (typically from in-sync  </span></span><br><span class="line"><span class="comment"> followers or consumers) are very close to the end of the index. So, the LRU cache replacement policy should work very  </span></span><br><span class="line"><span class="comment"> well with Kafka's index access pattern.  </span></span><br><span class="line"><span class="comment"> */</span>  </span><br><span class="line">   </span><br><span class="line"> <span class="meta">@volatile</span>  </span><br><span class="line"> <span class="keyword">protected</span> <span class="keyword">var</span> mmap: <span class="type">MappedByteBuffer</span> = {  </span><br><span class="line"> <span class="keyword">val</span> newlyCreated = file.createNewFile()  </span><br><span class="line"> <span class="comment">//创建RandomAccessFile  </span></span><br><span class="line"> <span class="keyword">val</span> raf = <span class="keyword">if</span> (writable) <span class="keyword">new</span> <span class="type">RandomAccessFile</span>(file, <span class="string">"rw"</span>) <span class="keyword">else</span> <span class="keyword">new</span> <span class="type">RandomAccessFile</span>(file, <span class="string">"r"</span>)  </span><br><span class="line"> </span><br><span class="line"> <span class="comment">//开始map 文件  </span></span><br><span class="line"> <span class="keyword">if</span> (writable)  </span><br><span class="line"> raf.getChannel.map(<span class="type">FileChannel</span>.<span class="type">MapMode</span>.<span class="type">READ_WRITE</span>, <span class="number">0</span>, _length)  </span><br><span class="line"> <span class="keyword">else</span>  </span><br><span class="line"> raf.getChannel.map(<span class="type">FileChannel</span>.<span class="type">MapMode</span>.<span class="type">READ_ONLY</span>, <span class="number">0</span>, _length)  </span><br><span class="line"> </span><br><span class="line"> }  </span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure>

<p>另外一个有趣的地方是kafka的munmap方法，实际上调用的是DirectByteBuffer的cleanr方法来关闭文件映射，使用方式比较hack。代码如下(截取自MappedByteBuffers.java):</p>
<figure class="highlight pgsql"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">private static MethodHandle unmapJava7Or8(MethodHandles.Lookup lookup) throws ReflectiveOperationException {  </span><br><span class="line"> <span class="comment">/* "Compile" a MethodHandle that is roughly equivalent to the following lambda:  </span></span><br><span class="line"><span class="comment"> *  </span></span><br><span class="line"><span class="comment"> * (ByteBuffer buffer) -&gt; {  </span></span><br><span class="line"><span class="comment"> *   sun.misc.Cleaner cleaner = ((java.nio.DirectByteBuffer) byteBuffer).cleaner();  </span></span><br><span class="line"><span class="comment"> *   if (nonNull(cleaner))  </span></span><br><span class="line"><span class="comment"> *     cleaner.clean();  </span></span><br><span class="line"><span class="comment"> *   else  </span></span><br><span class="line"><span class="comment"> *     noop(cleaner); // the noop is needed because MethodHandles#guardWithTest always needs both if and else  </span></span><br><span class="line"><span class="comment"> * }  </span></span><br><span class="line"><span class="comment"> */</span>  </span><br><span class="line"> <span class="keyword">Class</span>&lt;?&gt; directBufferClass = <span class="keyword">Class</span>.forName("java.nio.DirectByteBuffer");  </span><br><span class="line"> <span class="keyword">Method</span> m = directBufferClass.getMethod("cleaner");  </span><br><span class="line"> m.setAccessible(<span class="keyword">true</span>);  </span><br><span class="line"> MethodHandle directBufferCleanerMethod = lookup.unreflect(m);  </span><br><span class="line"> <span class="keyword">Class</span>&lt;?&gt; cleanerClass = directBufferCleanerMethod.<span class="keyword">type</span>().returnType();  </span><br><span class="line"> MethodHandle cleanMethod = lookup.findVirtual(cleanerClass, "clean", methodType(<span class="type">void</span>.<span class="keyword">class</span>));  </span><br><span class="line"> MethodHandle nonNullTest = lookup.findStatic(MappedByteBuffers.<span class="keyword">class</span>, "nonNull",  </span><br><span class="line"> methodType(<span class="type">boolean</span>.<span class="keyword">class</span>, <span class="keyword">Object</span>.<span class="keyword">class</span>)).asType(methodType(<span class="type">boolean</span>.<span class="keyword">class</span>, cleanerClass));  </span><br><span class="line"> MethodHandle noop = dropArguments(<span class="keyword">constant</span>(<span class="type">Void</span>.<span class="keyword">class</span>, <span class="keyword">null</span>).asType(methodType(<span class="type">void</span>.<span class="keyword">class</span>)), <span class="number">0</span>, cleanerClass);  </span><br><span class="line"> MethodHandle unmapper = filterReturnValue(directBufferCleanerMethod, guardWithTest(nonNullTest, cleanMethod, noop))  </span><br><span class="line"> .asType(methodType(<span class="type">void</span>.<span class="keyword">class</span>, ByteBuffer.<span class="keyword">class</span>));  </span><br><span class="line"> <span class="keyword">return</span> unmapper;  </span><br><span class="line"> }  </span><br></pre></td></tr></tbody></table></figure>
<p>参考:</p>
<ol>
<li>nachoparker 2018 <a target="_blank" rel="noopener" href="https://ownyourbits.com/2018/05/02/understanding-disk-usage-in-linux/">https://ownyourbits.com/2018/05/02/understanding-disk-usage-in-linux/</a></li>
<li>Lokesh Gupta , <a target="_blank" rel="noopener" href="https://howtodoinjava.com/java/io/how-java-io-works-internally-at-lower-level/">https://howtodoinjava.com/java/io/how-java-io-works-internally-at-lower-level/</a></li>
<li>Shawn Xu, <a target="_blank" rel="noopener" href="https://medium.com/@xunnan.xu/its-all-about-buffers-zero-copy-mmap-and-java-nio-50f2a1bfc05c">https://medium.com/@xunnan.xu/its-all-about-buffers-zero-copy-mmap-and-java-nio-50f2a1bfc05c</a></li>
<li><a target="_blank" rel="noopener" href="https://stackoverflow.com/questions/8263995/standardopenoption-sync-vs-standardopenoption-dsync">https://stackoverflow.com/questions/8263995/standardopenoption-sync-vs-standardopenoption-dsync</a></li>
<li><a target="_blank" rel="noopener" href="https://stackoverflow.com/questions/4072878/i-o-concept-flush-vs-sync">https://stackoverflow.com/questions/4072878/i-o-concept-flush-vs-sync</a></li>
</ol>
</div><div class="post-copyright"><script type="text/javascript" src="/js/copyright.js?v=1.0.0" successtext="复制成功！"></script><link rel="stylesheet" type="text/css" href="/css/copyright.css?v=1.0.0"><p><span>本文标题：</span>磁盘读写优化技术研究-Zookeeper/Kafka/MySQL</p><p><span>文章作者：</span>王军飞 jonefeewang@outlook.com</p><p><span>发布时间：</span>2020-03-15</p><p><span>最后更新：</span>2024-03-24</p><p><span>原始链接：</span><a href="/2020/03/15/磁盘读写优化技术研究-Zookeeper-Kafka-MySQL/">https://www.wangjunfei.com/2020/03/15/%E7%A3%81%E7%9B%98%E8%AF%BB%E5%86%99%E4%BC%98%E5%8C%96%E6%8A%80%E6%9C%AF%E7%A0%94%E7%A9%B6-Zookeeper-Kafka-MySQL/</a><span class="copy-path"><i class="fa fa-clipboard" data-clipboard-text="https://www.wangjunfei.com/2020/03/15/%E7%A3%81%E7%9B%98%E8%AF%BB%E5%86%99%E4%BC%98%E5%8C%96%E6%8A%80%E6%9C%AF%E7%A0%94%E7%A9%B6-Zookeeper-Kafka-MySQL/"></i></span></p><p><span>版权声明：</span>原创文章转载请注明出处</p></div><br><script type="text/javascript" src="/js/share.js?v=1.0.0" async></script><a class="article-share-link" data-url="https://www.wangjunfei.com/2020/03/15/%E7%A3%81%E7%9B%98%E8%AF%BB%E5%86%99%E4%BC%98%E5%8C%96%E6%8A%80%E6%9C%AF%E7%A0%94%E7%A9%B6-Zookeeper-Kafka-MySQL/" data-id="clu6b2h97001whzs65cfy3g9l" data-qrcode="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAASYAAAEmCAAAAADqr2IGAAAEf0lEQVR42u3aS27kOBAFQN//0h5gMLtpl9/LZBtFObQqoFsSGTLA/H18xNfnv9dXv1//6/9/509+vZ7XV7LCwxcmTJgwYXpLpvxl+XNyjpa+BU1W8nrvmDBhwoTpGUz545K7kk22rK+xEpRZmIIJEyZMmH4PU/uaWaiRc8xCB0yYMGHChKk9ks8myfuVJMktJkyYMGH6bUxnm4KzRmaeDOdAM0RMmDBhwvQkpv3gzr2/f3S+CRMmTJgw/TjTZ3m1d+0bnPskdrPf/56JCRMmTJiuZdq3D9snbAKR5GiffbCIGxMmTJgwXcj0M6XY/FDPm51JmLIfM8KECRMmTM9myg/O2TDNqUGcWeBSBBOYMGHChOlypnyhyf/PN9l+kjxQmIUIX64KEyZMmDBdyzQbamlf05ZZZ2n5JkmOStKYMGHChOlyprxVOSvpnhobzYvCp4aHMGHChAnTM5iSVmLbyNyUZdtkeAZd/IlgwoQJE6ZrmdrU8dQROxuyaT/PrExcj9hiwoQJE6Y3ZkrafptRnryAuxnWmZWMiw+DCRMmTJiuZZqloLNW4qkWZp6En02bMWHChAnTjUyzAdDXr3y9lGIwdHSE5zT5IBEmTJgwYbqRqV1iPjpzKlzYlIlnjdg/vBETJkyYMF3L1Cai7SJmqWw+GJQf822pFxMmTJgwPYMpbx/miHm7MT/sZwXf/F3fhBeYMGHChOkRTG3KOiv1foyu/N79ezFhwoQJ0/OY8s0nS5k1R/PG56ZVmb8LEyZMmDD9Hqa2XDs71BOaTWk4L+8Wi8OECRMmTJcw1f3PEmJ/2O8HffJ9/WEXmDBhwoTpWqb9dfbwbtPj129vi9f5SCsmTJgwYbqFKS/yni255ts7xTcbIcKECRMmTLcztTfMxkb3ZKfGVVdjRpgwYcKE6RFMbZtz1sjMS7GzrH1WPj5QEceECRMmTG/JlLck88JocrTnz9mM+CR/EHUBGhMmTJgwXcKULCgPCPJjNVlc3hydtUWLz4MJEyZMmC5nmo255Ed+C90Walel2wQUEyZMmDBdy9QOa+bDPasSavmcvzG4UwBhwoQJE6a3Z5qFBTO+TdPxVPOyHeXBhAkTJkz3Mu0P+FmgkB/hbUSThwJFExcTJkyYMF3LNBvcabcxa2q2bzxVXC7qAZgwYcKE6e2ZNunohmbf5kwCl7ZdGnV9MWHChAnTtUx50bYd2Wk306bceUtylnhjwoQJE6ZnMCUJ4dkkdtaw3HPXTVxMmDBhwnQtUzuOM2sczsZxcspZw3WGiwkTJkyY7mL6LK+2yJsvPS+/tiXmvIX55V2YMGHChOlaps2RvG9wbsZ08kBhluhiwoQJE6YnMc2afO2hOwsIkuCgLQS3LU9MmDBhwvQMpk1TcLOUNijJk+Q2AvomiMGECRMmTL+AaZOyznBnA0CzQGHYU8WECRMmTA9lykdh9gNA0YEdpNCrKjgmTJgwYbqcqR3Wac/NNok99THyUvI3fwqYMGHChOlapn3et9l22zpt703WOUTBhAkTJkzvzvQPZiN3mJ5TI3gAAAAASUVORK5CYII=">分享</a><div class="tags"><ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/kafka/" rel="tag">kafka</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/mysql/" rel="tag">mysql</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/zookeeper/" rel="tag">zookeeper</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E7%A3%81%E7%9B%98%E8%AF%BB%E5%86%99%E4%BC%98%E5%8C%96/" rel="tag">磁盘读写优化</a></li></ul></div><div class="post-nav"><a class="pre" href="/2020/04/28/RocketMQ%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97-%E4%B8%80-%E6%9E%B6%E6%9E%84/">RocketMQ消息队列(一)架构</a><a class="next" href="/2020/02/26/%E4%B8%80%E8%87%B4%E6%80%A7%E7%AE%97%E6%B3%95%E7%A0%94%E7%A9%B6-%E4%B8%89-Kafka/">一致性算法研究(三)Kafka</a></div><div class="nofancybox" id="vcomment"></div><script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script><script src="//unpkg.com/valine@latest/dist/Valine.min.js"></script><script>var notify = 'false' == 'true' ? true : false;
var verify = 'true' == 'true' ? true : false;
var GUEST_INFO = ['nick','mail','link'];
var guest_info = 'nick,mail,link'.split(',').filter(function(item){
  return GUEST_INFO.indexOf(item) > -1
});
guest_info = guest_info.length == 0 ? GUEST_INFO :guest_info;
window.valine = new Valine({
  el:'#vcomment',
  notify:notify,
  verify:verify,
  appId:'FNjCNRwQqC3sTPP04DfRFIk9-gzGzoHsz',
  appKey:'LzCq5t6a6RTkwXQVr8xLk0yj',
  serverURLs:'',
  placeholder:'请理性评论',
  avatar:'monsterid',
  guest_info:guest_info,
  pageSize:'10'
})
</script></div></div></div><div class="pure-u-1 pure-u-md-1-4"><div id="sidebar"><div class="widget"><form class="search-form" action="//www.baidu.com/baidu" method="get" accept-charset="utf-8" target="_blank"><input type="search" name="word" maxlength="20" placeholder="Search"/><input type="hidden" name="si" value="https://www.wangjunfei.com"/><input name="tn" type="hidden" value="bds"/><input name="cl" type="hidden" value="3"/><input name="ct" type="hidden" value="2097152"/><input name="s" type="hidden" value="on"/></form></div><div class="widget"><div class="author-info"><a class="info-avatar" href="/about/" title="关于"><img class="nofancybox" src="/img/avatar.png"/></a><p>Always be the best</p><a class="info-icon" href="mailto:jonefeewang@outlook.com" title="Email" target="_blank" style="margin-inline:5px"> <i class="fa fa-envelope-square" style="margin-inline:5px"></i></a><a class="info-icon" href="https://github.com/jonefeewang" title="Github" target="_blank" style="margin-inline:5px"> <i class="fa fa-github-square" style="margin-inline:5px"></i></a><a class="info-icon" href="/atom.xml" title="RSS" target="_blank" style="margin-inline:5px"> <i class="fa fa-rss-square" style="margin-inline:5px"></i></a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> 分类</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%88%86%E5%B8%83%E5%BC%8F%E6%8A%80%E6%9C%AF/">分布式技术</a><span class="category-list-count">3</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%8A%A1%E8%99%9A-%E4%BA%A4%E6%B5%81%E6%B2%9F%E9%80%9A/">务虚-交流沟通</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%8A%A1%E8%99%9A-%E7%AE%A1%E7%90%86%E6%96%B9%E6%B3%95%E8%AE%BA/">务虚-管理方法论</a><span class="category-list-count">8</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/">消息队列</a><span class="category-list-count">9</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E7%A3%81%E7%9B%98%E8%AF%BB%E5%86%99%E4%BC%98%E5%8C%96/">磁盘读写优化</a><span class="category-list-count">1</span></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> 标签</i></div><div class="tagcloud"><a href="/tags/Apache-pulsar-%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97-kafka-RocketMQ/" style="font-size: 15px;">Apache pulsar 消息队列 kafka RocketMQ</a> <a href="/tags/kafka-raft-kraft-zookeeper/" style="font-size: 15px;">kafka raft kraft zookeeper</a> <a href="/tags/%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/" style="font-size: 15px;">消息队列</a> <a href="/tags/rocketmq/" style="font-size: 15px;">rocketmq</a> <a href="/tags/kafka/" style="font-size: 15px;">kafka</a> <a href="/tags/pulsar/" style="font-size: 15px;">pulsar</a> <a href="/tags/Apache-Pulsar/" style="font-size: 15px;">Apache Pulsar</a> <a href="/tags/Apache-Bookkeeper/" style="font-size: 15px;">Apache Bookkeeper</a> <a href="/tags/Apache-Kafka/" style="font-size: 15px;">Apache Kafka</a> <a href="/tags/%E5%8A%A1%E8%99%9A-%E7%AE%A1%E7%90%86%E6%96%B9%E6%B3%95%E8%AE%BA/" style="font-size: 15px;">务虚-管理方法论</a> <a href="/tags/%E5%8D%8E%E4%B8%BA/" style="font-size: 15px;">华为</a> <a href="/tags/%E7%AE%A1%E7%90%86%E6%96%B9%E6%B3%95%E8%AE%BA/" style="font-size: 15px;">管理方法论</a> <a href="/tags/%E7%AE%A1%E7%90%86/" style="font-size: 15px;">管理</a> <a href="/tags/%E5%8A%A1%E8%99%9A-%E9%A2%86%E5%AF%BC%E6%A2%AF%E9%98%9F/" style="font-size: 15px;">务虚-领导梯队</a> <a href="/tags/%E5%8A%A1%E8%99%9A/" style="font-size: 15px;">务虚</a> <a href="/tags/%E5%8D%8F%E4%BD%9C/" style="font-size: 15px;">协作</a> <a href="/tags/%E8%BD%AF%E7%B4%A0%E8%B4%A8/" style="font-size: 15px;">软素质</a> <a href="/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E6%8A%80%E6%9C%AF/" style="font-size: 15px;">分布式技术</a> <a href="/tags/zookeeper/" style="font-size: 15px;">zookeeper</a> <a href="/tags/consensus-algorithm/" style="font-size: 15px;">consensus algorithm</a> <a href="/tags/zab/" style="font-size: 15px;">zab</a> <a href="/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E5%8D%8F%E8%AE%AE/" style="font-size: 15px;">分布式协议</a> <a href="/tags/%E4%B8%80%E8%87%B4%E6%80%A7%E7%AE%97%E6%B3%95/" style="font-size: 15px;">一致性算法</a> <a href="/tags/%E6%96%B9%E6%B3%95%E8%AE%BA-%E8%BD%AF%E7%B4%A0%E8%B4%A8%E6%8F%90%E5%8D%87/" style="font-size: 15px;">方法论 软素质提升</a> <a href="/tags/kafka-netty-%E6%97%B6%E9%97%B4%E8%BD%AE/" style="font-size: 15px;">kafka netty 时间轮</a> <a href="/tags/%E7%A3%81%E7%9B%98%E8%AF%BB%E5%86%99%E4%BC%98%E5%8C%96/" style="font-size: 15px;">磁盘读写优化</a> <a href="/tags/mysql/" style="font-size: 15px;">mysql</a> <a href="/tags/%E4%BA%91%E5%8E%9F%E7%94%9F/" style="font-size: 15px;">云原生</a> <a href="/tags/%E6%B5%81%E5%BC%8F%E8%AE%A1%E7%AE%97/" style="font-size: 15px;">流式计算</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> 最近文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2023/12/25/Apache-Pulsar%E7%9A%84%E5%AD%98%E5%82%A8%E6%A8%A1%E5%9E%8B/">Apache Pulsar的存储模型</a></li><li class="post-list-item"><a class="post-list-link" href="/2023/09/15/Apache-pulsar%E5%AF%B9zookeeper%E7%9A%84%E4%BE%9D%E8%B5%96%E5%92%8C%E5%8E%BB%E9%99%A4%E5%88%86%E6%9E%90-%E4%BA%8C/">Apache pulsar对zookeeper的依赖和去除分析(二)</a></li><li class="post-list-item"><a class="post-list-link" href="/2023/06/15/Apache-pulsar%E5%AF%B9zookeeper%E7%9A%84%E4%BE%9D%E8%B5%96%E5%92%8C%E5%8E%BB%E9%99%A4%E5%88%86%E6%9E%90-%E4%B8%80/">Apache pulsar对zookeeper的依赖和去除分析(一)</a></li><li class="post-list-item"><a class="post-list-link" href="/2023/04/10/%E6%97%B6%E9%97%B4%E8%BD%AE%E7%AE%97%E6%B3%95%E5%88%86%E6%9E%90%E5%92%8C%E5%BA%94%E7%94%A8/">时间轮算法分析和应用</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/12/01/Kafka%E5%8E%BBZookeeper%E6%8F%AD%E7%A7%98/">Kafka去Zookeeper揭秘</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/09/15/PRFAQ%E7%BC%96%E5%86%99%E8%A7%84%E8%8C%83/">「转」PRFAQ编写规范</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/09/13/%E5%87%A0%E7%A7%8D%E5%B8%B8%E8%A7%81%E7%9A%84%E5%B7%A5%E4%BD%9C%E6%96%B9%E6%B3%95%E8%AE%BA%E6%B1%87%E9%9B%86/">「转」几种常见的工作方法论汇集</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/08/14/%E8%BD%AF%E7%B4%A0%E8%B4%A8%E8%9E%BA%E6%97%8B%E4%B8%8A%E5%8D%87/">软素质螺旋上升</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/07/22/Mafka%E5%85%A8%E9%93%BE%E8%B7%AF%E5%BC%B9%E6%80%A7%E4%BC%B8%E7%BC%A9%E6%BC%94%E8%BF%9B%E7%AD%96%E7%95%A5/">Mafka全链路弹性伸缩演进策略</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/07/14/Mafka-LRP/">Mafka长期发展规划(LRP)</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-external-link"> 友情链接</i></div><ul></ul><a href="http://www.example1.com/" title="site-name1" target="_blank">site-name1</a></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2024 <a href="/." rel="nofollow">王军飞的随笔.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a class="show" id="rocket" href="#top"></a><script type="text/javascript" src="/js/totop.js?v=1.0.0" async></script><script type="text/javascript" src="//lib.baomitu.com/fancybox/latest/jquery.fancybox.min.js"></script><script type="text/javascript" src="/js/fancybox.js?v=1.0.0"></script><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/fancybox/latest/jquery.fancybox.min.css"><script type="text/javascript" src="/js/copycode.js?v=1.0.0" successtext="复制成功！"></script><link rel="stylesheet" type="text/css" href="/css/copycode.css?v=1.0.0"><script type="text/javascript" src="/js/codeblock-resizer.js?v=1.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=1.0.0"></script></div></body></html>