<!DOCTYPE html><html lang="en-US"><head><meta name="google-site-verification" content="IqPxX6jMzoqSIvuvpvuP9PCF7niBjR9EBY0UfP9MHEU"><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content="Rex Wangs blog"><title>Rewriting Kafka in Rust Async: Insights and Lessons Learned in Rust | Rex Wang</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=1.0.0"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/normalize/latest/normalize.min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/latest/pure-min.min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/latest/grids-responsive-min.min.css"><link rel="stylesheet" href="//lib.baomitu.com/font-awesome/4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//lib.baomitu.com/jquery/latest/jquery.min.js"></script><link rel="icon" mask="" sizes="any" href="/favicon.ico"><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"><link rel="alternate" type="application/atom+xml" href="/atom.xml"><script type="text/javascript" src="//lib.baomitu.com/clipboard.js/latest/clipboard.min.js"></script><script type="text/javascript" src="//lib.baomitu.com/toastr.js/latest/toastr.min.js"></script><link rel="stylesheet" href="//lib.baomitu.com/toastr.js/latest/toastr.min.css"><meta name="generator" content="Hexo 7.3.0">
<style>.github-emoji { position: relative; display: inline-block; width: 1.2em; min-height: 1.2em; overflow: hidden; vertical-align: top; color: transparent; }  .github-emoji > span { position: relative; z-index: 10; }  .github-emoji img, .github-emoji .fancybox { margin: 0 !important; padding: 0 !important; border: none !important; outline: none !important; text-decoration: none !important; user-select: none !important; cursor: auto !important; }  .github-emoji img { height: 1.2em !important; width: 1.2em !important; position: absolute !important; left: 50% !important; top: 50% !important; transform: translate(-50%, -50%) !important; user-select: none !important; cursor: auto !important; } .github-emoji-fallback { color: inherit; } .github-emoji-fallback img { opacity: 0 !important; }</style>
</head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">Rewriting Kafka in Rust Async: Insights and Lessons Learned in Rust</h1><a id="logo" href="/.">Rex Wang</a><p class="description">always be the best</p></div><div id="nav-menu"><a class="current" href="/."><i class="fa fa-home"> Start</i></a><a href="/archives/"><i class="fa fa-archive"> Archiv</i></a><a href="/about/"><i class="fa fa-user"> Über</i></a><a href="/atom.xml"><i class="fa fa-rss"> RSS</i></a></div></div><div class="pure-g" id="layout"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">Rewriting Kafka in Rust Async: Insights and Lessons Learned in Rust</h1><div class="post-meta">2025-06-18<script src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" async></script><span id="busuanzi_container_page_pv"> | <span id="busuanzi_value_page_pv"></span><span> Hits</span></span><span class="post-time"><span class="post-meta-item-text"> | </span><span class="post-meta-item-icon"><i class="fa fa-keyboard-o"></i><span class="post-count"> 3.8k</span><span class="post-meta-item-text"> Words</span></span></span><span class="post-time"> | <span class="post-meta-item-icon"><i class="fa fa-clock-o"></i><span class="post-count"> 23</span><span class="post-meta-item-text"> Minutes</span></span></span></div><a class="disqus-comment-count" href="/2025/06/18/Rewriting-Kafka-in-Rust-Async-Insights-and-Lessons-Learned/#vcomment"><span class="valine-comment-count" data-xid="/2025/06/18/Rewriting-Kafka-in-Rust-Async-Insights-and-Lessons-Learned/"></span><span> Comment</span></a><div class="clear"><div class="toc-article" id="toc"><div class="toc-title">Inhalte</div><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#TL-DR"><span class="toc-number">1.</span> <span class="toc-text">TL;DR:</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-Avoid-Turning-Functions-into-async-Whenever-Possible"><span class="toc-number">2.</span> <span class="toc-text">1. Avoid Turning Functions into async Whenever Possible</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-Minimize-the-Number-of-Tokio-Tasks"><span class="toc-number">3.</span> <span class="toc-text">2. Minimize the Number of Tokio Tasks</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-1-Design-Experience-Efficient-Task-Management-and-Request-Handling-in-StoneMQ"><span class="toc-number">3.1.</span> <span class="toc-text">2.1 Design Experience: Efficient Task Management and Request Handling in StoneMQ</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#The-Problem-with-Excessive-Task-Spawning"><span class="toc-number">3.1.1.</span> <span class="toc-text">The Problem with Excessive Task Spawning</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#My-Approach-Connection-per-Task-Centralized-Request-Processing"><span class="toc-number">3.1.2.</span> <span class="toc-text">My Approach: Connection-per-Task, Centralized Request Processing</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Automatic-Worker-Recovery"><span class="toc-number">3.1.3.</span> <span class="toc-text">Automatic Worker Recovery</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Benefits"><span class="toc-number">3.1.4.</span> <span class="toc-text">Benefits</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Conclusion"><span class="toc-number">3.1.5.</span> <span class="toc-text">Conclusion</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-Design-Experience-Favoring-Lock-Free-Architectures-and-Minimizing-Use-of-Tokio-Async-Locks"><span class="toc-number">4.</span> <span class="toc-text">3. Design Experience: Favoring Lock-Free Architectures and Minimizing Use of Tokio Async Locks</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Why-Avoid-Locks-Especially-Async-Locks"><span class="toc-number">4.0.1.</span> <span class="toc-text">Why Avoid Locks, Especially Async Locks?</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#My-Approach-Channels-Message-Passing-and-Task-Ownership"><span class="toc-number">4.0.2.</span> <span class="toc-text">My Approach: Channels, Message Passing, and Task Ownership</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Example-Journal-Log-Write-Path"><span class="toc-number">4.0.3.</span> <span class="toc-text">Example: Journal Log Write Path</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Benefits-1"><span class="toc-number">4.0.4.</span> <span class="toc-text">Benefits</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Conclusion-1"><span class="toc-number">4.0.5.</span> <span class="toc-text">Conclusion</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-Design-Experience-Judicious-Use-of-Unsafe-Code-for-Performance-Critical-Paths"><span class="toc-number">5.</span> <span class="toc-text">4. Design Experience: Judicious Use of Unsafe Code for Performance-Critical Paths</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Why-Use-Unsafe-Code"><span class="toc-number">5.0.1.</span> <span class="toc-text">Why Use Unsafe Code?</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#My-Approach-Isolate-and-Audit-Unsafe-Code"><span class="toc-number">5.0.2.</span> <span class="toc-text">My Approach: Isolate and Audit Unsafe Code</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Example-Memory-Mapped-Index-Files"><span class="toc-number">5.0.3.</span> <span class="toc-text">Example: Memory-Mapped Index Files</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Benefits-2"><span class="toc-number">5.0.4.</span> <span class="toc-text">Benefits</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Conclusion-2"><span class="toc-number">5.0.5.</span> <span class="toc-text">Conclusion</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-Design-Experience-Separating-Mutable-and-Immutable-Data-to-Optimize-Lock-Granularity"><span class="toc-number">6.</span> <span class="toc-text">5. Design Experience: Separating Mutable and Immutable Data to Optimize Lock Granularity</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Why-Separate-Mutable-and-Immutable-Data"><span class="toc-number">6.0.1.</span> <span class="toc-text">Why Separate Mutable and Immutable Data?</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#My-Approach-Example-from-Journal-Log-Management"><span class="toc-number">6.0.2.</span> <span class="toc-text">My Approach: Example from Journal Log Management</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Benefits-3"><span class="toc-number">6.0.3.</span> <span class="toc-text">Benefits</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Conclusion-3"><span class="toc-number">6.0.4.</span> <span class="toc-text">Conclusion</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-Separate-Asynchronous-and-Synchronous-Data-Operations-to-Optimize-Lock-Usage"><span class="toc-number">7.</span> <span class="toc-text">6. Separate Asynchronous and Synchronous Data Operations to Optimize Lock Usage</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Example-StoneMQ-Journal-Log-Write-Path"><span class="toc-number">7.0.1.</span> <span class="toc-text">Example: StoneMQ Journal Log Write Path</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Example-Group-Coordinator"><span class="toc-number">7.0.2.</span> <span class="toc-text">Example: Group Coordinator</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Benefits-4"><span class="toc-number">7.1.</span> <span class="toc-text">Benefits</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Conclusion-4"><span class="toc-number">7.2.</span> <span class="toc-text">Conclusion</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#7-Employ-Static-Dispatch-in-Performance-Critical-Paths-Whenever-Possible"><span class="toc-number">8.</span> <span class="toc-text">7. Employ Static Dispatch in Performance-Critical Paths Whenever Possible</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Design-Experience-Using-Enums-for-Protocol-Type-Polymorphism-in-Performance-Critical-Code"><span class="toc-number">8.1.</span> <span class="toc-text">Design Experience: Using Enums for Protocol Type Polymorphism in Performance-Critical Code</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Why-Not-Trait-Objects"><span class="toc-number">8.1.1.</span> <span class="toc-text">Why Not Trait Objects?</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#The-Enum-Approach"><span class="toc-number">8.1.2.</span> <span class="toc-text">The Enum Approach</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Benefits-in-Practice"><span class="toc-number">8.1.3.</span> <span class="toc-text">Benefits in Practice</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Example-Array-Construction"><span class="toc-number">8.1.4.</span> <span class="toc-text">Example: Array Construction</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Conclusion-5"><span class="toc-number">8.1.5.</span> <span class="toc-text">Conclusion</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Summary"><span class="toc-number">9.</span> <span class="toc-text">Summary</span></a></li></ol></div></div><div class="post-content"><h2 id="TL-DR"><a href="#TL-DR" class="headerlink" title="TL;DR:"></a>TL;DR:</h2><ol>
<li>Rewriting Kafka in Rust not only leverages Rust’s language advantages but also allows redesigning for superior performance and efficiency.</li>
<li>Design Experience: Avoid Turning Functions into async Whenever Possible</li>
<li>Design Experience: Minimize the Number of Tokio Tasks</li>
<li>Design Experience: Judicious Use of Unsafe Code for Performance-Critical Paths</li>
<li>Design Experience: Separating Mutable and Immutable Data to Optimize Lock Granularity</li>
<li>Design Experience: Separate Asynchronous and Synchronous Data Operations to Optimize Lock Usage</li>
<li>Design Experience: Employ Static Dispatch in Performance-Critical Paths Whenever Possible</li>
</ol>
<p>At the project’s inception, I initially considered implementing <a target="_blank" rel="noopener" href="https://github.com/jonefeewang/stonemq">StoneMQ</a> using C/C++. After grappling with the frustrating and persistent off-heap memory leak issues encountered in DDMQ (built on Pulsar) within the JVM environment, I realized that relying on off-heap memory for caching or buffering in the JVM is far from ideal. Having operated the Mafka cluster (based on Kafka) for nearly five years, we never faced memory-related problems such as abnormal memory growth, prolonged garbage collection times, or memory leaks. When a memory leak occurs on a single node in a cluster, memory usage escalates uncontrollably, leaving you no choice but to perform periodic restarts—a daunting and risky task when managing thousands of nodes.</p>
<p>Kafka and Pulsar both run as services on Linux; more precisely, they operate within the JVM and have no direct interaction with the native Linux system. Strictly speaking, they do not belong to the domain of system-level programming. So why choose C/C++ as the underlying language? This choice clearly results in reduced development efficiency and increased complexity.  </p>
<p>My perspective is that Kafka and Pulsar, as messaging queues, have become de facto industrial standards, having matured and stabilized over nearly eight years. Could we not reimplement them in a lower-level language to reap greater benefits in performance and stability? With some enhancements, might we transform them into first-class native Linux processes, achieving a level of integration and efficiency akin to that of the Linux kernel’s own services?</p>
<p>Through extensive research, it has become evident that C/C++ is gradually becoming a sunset language, nearing its twilight. Its successor, Rust, which emerged around 2010, has spent over a decade steadily conquering the domain of systems programming. Even the notoriously exacting Linus Torvalds could no longer ignore Rust’s presence; in a recent kernel update, he resolutely overcame opposition from the C community to formally integrate Rust into the Linux kernel mainline for the first time.  </p>
<p>Google has employed Rust for years in Android’s low-level development, claiming efficiency and safety several times superior to C++. The CTO of Microsoft Azure has even recommended that new projects move away from C/C++ in favor of Rust. Amazon AWS, notably, has recruited key Rust contributors and utilized Rust to build innovative projects such as the new S3 backend storage and the renowned Firecracker microVM.  </p>
<p>Against this backdrop, I decisively chose Rust as the development language for StoneMQ. Admittedly, Rust has a steep learning curve, a common critique among developers. However, while challenging, it is by no means insurmountable. Moreover, system developers constitute a relatively small niche within the broader developer community, so widespread adoption is not imperative.  </p>
<p>After a year of iterative development—learning Rust while coding—StoneMQ has encountered and anticipated numerous pitfalls. Here, I summarize some of these lessons, with plans to elaborate further in the future, hoping to aid others in avoiding similar missteps. (Repository: <a target="_blank" rel="noopener" href="https://github.com/jonefeewang/stonemq">https://github.com/jonefeewang/stonemq</a>)  </p>
<p>Though StoneMQ was developed concurrently with my Rust learning journey, it was built to exacting standards: first, to outperform Kafka in terms of performance; second, to maintain clear and readable code. Achieving these goals demanded meticulous attention to design and optimization throughout development. StoneMQ underwent two major restructurings and numerous minor refactorings—details of which can be explored through its extensive git commit history.</p>
<h2 id="1-Avoid-Turning-Functions-into-async-Whenever-Possible"><a href="#1-Avoid-Turning-Functions-into-async-Whenever-Possible" class="headerlink" title="1. Avoid Turning Functions into async Whenever Possible"></a>1. Avoid Turning Functions into async Whenever Possible</h2><p>In Rust, declaring a function as <code>async fn</code> compiles it into a state machine, and its invocation yields a Future. While asynchronous functions enable more concise code, excessive use of <code>async</code>—especially when applied to logic that could be executed synchronously—results in:</p>
<ul>
<li>Additional overhead from the Future state machine;</li>
<li>Proliferation of tasks (e.g., Tokio tasks);</li>
<li>Increased load on the backend scheduler.</li>
</ul>
<p>Therefore, the best practice when designing asynchronous interfaces is to <strong>prefer synchronous functions for logic that can run synchronously, reserving async functions solely for genuinely asynchronous operations.</strong>  </p>
<p>If only a portion of a function requires asynchronous handling, separate the synchronous logic from the asynchronous part. This approach allows finer control over task granularity and scheduling overhead, ultimately enhancing overall efficiency and responsiveness.</p>
<p>Consider the following example:</p>
<figure class="highlight rust"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// INEFFICIENT: Entire function is async, though only the IO operations require</span></span><br><span class="line"><span class="keyword">async</span> <span class="keyword">fn</span> <span class="title function_">fetch_and_parse</span>(url: &amp;<span class="type">str</span>) <span class="punctuation">-&gt;</span> anyhow::<span class="type">Result</span>&lt;Data&gt; {</span><br><span class="line">    <span class="keyword">let</span> <span class="variable">body</span> = reqwest::<span class="title function_ invoke__">get</span>(url).<span class="keyword">await</span>?.<span class="title function_ invoke__">text</span>().<span class="keyword">await</span>?;</span><br><span class="line">    <span class="keyword">let</span> <span class="variable">parsed</span> = serde_json::from_str::&lt;Data&gt;(&amp;body)?;</span><br><span class="line">    <span class="title function_ invoke__">Ok</span>(parsed)</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="comment">// EFFICIENT: Separate synchronous parsing from asynchronous downloading</span></span><br><span class="line"><span class="keyword">async</span> <span class="keyword">fn</span> <span class="title function_">fetch_and_parse</span>(url: &amp;<span class="type">str</span>) <span class="punctuation">-&gt;</span> anyhow::<span class="type">Result</span>&lt;Data&gt; {</span><br><span class="line">    <span class="keyword">let</span> <span class="variable">body</span> = reqwest::<span class="title function_ invoke__">get</span>(url).<span class="keyword">await</span>?.<span class="title function_ invoke__">text</span>().<span class="keyword">await</span>?;</span><br><span class="line">    <span class="title function_ invoke__">Ok</span>(<span class="title function_ invoke__">parse_json</span>(&amp;body)?)              <span class="comment">// parse_json is sync</span></span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="keyword">fn</span> <span class="title function_">parse_json</span>(raw: &amp;<span class="type">str</span>) <span class="punctuation">-&gt;</span> anyhow::<span class="type">Result</span>&lt;Data&gt; {</span><br><span class="line">    <span class="title function_ invoke__">Ok</span>(serde_json::<span class="title function_ invoke__">from_str</span>(raw)?)</span><br><span class="line">}</span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure>

<h2 id="2-Minimize-the-Number-of-Tokio-Tasks"><a href="#2-Minimize-the-Number-of-Tokio-Tasks" class="headerlink" title="2. Minimize the Number of Tokio Tasks"></a>2. Minimize the Number of Tokio Tasks</h2><p>The Tokio runtime manages asynchronous operations by scheduling tasks across CPU cores. Excessive task counts can cause several issues:</p>
<ul>
<li>Frequent task switching leads to CPU cache thrashing, degrading pipeline efficiency;</li>
<li>The scheduler may engage in “busy waiting” among numerous idle tasks, wasting CPU cycles;</li>
<li>Excessive task preemption slows response times and increases latency.</li>
</ul>
<p>Therefore, judiciously consolidating tasks and avoiding the creation of numerous tiny asynchronous tasks is crucial for optimizing Tokio’s scheduling efficiency. By controlling the number of tasks, the scheduler’s overhead is reduced, allowing critical tasks to receive more CPU time slices, thereby improving system throughput and latency.</p>
<p><strong>Example code:</strong></p>
<figure class="highlight rust"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Excessive small tasks: spawning a task per record</span></span><br><span class="line"><span class="keyword">async</span> <span class="keyword">fn</span> <span class="title function_">process_many</span>(records: <span class="type">Vec</span>&lt;<span class="type">String</span>&gt;) {</span><br><span class="line">    <span class="keyword">for</span> <span class="variable">r</span> <span class="keyword">in</span> records {</span><br><span class="line">        tokio::<span class="title function_ invoke__">spawn</span>(<span class="keyword">async</span> <span class="keyword">move</span> {</span><br><span class="line">            <span class="title function_ invoke__">process_record</span>(r).<span class="keyword">await</span>;</span><br><span class="line">        });</span><br><span class="line">    }</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>

<p>Reduce task count by batching processing:</p>
<figure class="highlight rust"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">async</span> <span class="keyword">fn</span> <span class="title function_">process_many</span>(records: <span class="type">Vec</span>&lt;<span class="type">String</span>&gt;) {</span><br><span class="line">    <span class="comment">// Spawn a single task to process multiple records in batch</span></span><br><span class="line">    tokio::<span class="title function_ invoke__">spawn</span>(<span class="keyword">async</span> <span class="keyword">move</span> {</span><br><span class="line">        <span class="keyword">for</span> <span class="variable">r</span> <span class="keyword">in</span> records {</span><br><span class="line">            <span class="title function_ invoke__">process_record</span>(r).<span class="keyword">await</span>;</span><br><span class="line">        }</span><br><span class="line">    });</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>

<p>Alternatively, design a pipeline using asynchronous channels to further reduce the number of tasks.</p>
<h3 id="2-1-Design-Experience-Efficient-Task-Management-and-Request-Handling-in-StoneMQ"><a href="#2-1-Design-Experience-Efficient-Task-Management-and-Request-Handling-in-StoneMQ" class="headerlink" title="2.1 Design Experience: Efficient Task Management and Request Handling in StoneMQ"></a>2.1 Design Experience: Efficient Task Management and Request Handling in StoneMQ</h3><p>When designing the server architecture for StoneMQ, I paid special attention to how asynchronous tasks (<code>tokio::task</code>) are managed, aiming to maximize resource efficiency and system stability.</p>
<h4 id="The-Problem-with-Excessive-Task-Spawning"><a href="#The-Problem-with-Excessive-Task-Spawning" class="headerlink" title="The Problem with Excessive Task Spawning"></a>The Problem with Excessive Task Spawning</h4><p>A common but problematic pattern in async server design is to spawn a new task for every incoming request. While this approach is simple, it can quickly lead to resource exhaustion under high load, as each request consumes memory and scheduling overhead. This can degrade performance and even cause the server to become unresponsive.</p>
<h4 id="My-Approach-Connection-per-Task-Centralized-Request-Processing"><a href="#My-Approach-Connection-per-Task-Centralized-Request-Processing" class="headerlink" title="My Approach: Connection-per-Task, Centralized Request Processing"></a>My Approach: Connection-per-Task, Centralized Request Processing</h4><p>Instead of spawning a task for every request, I designed the server so that:</p>
<ul>
<li><p><strong>Each client connection gets a single dedicated task</strong>:<br>In the code, every accepted TCP connection is handled by a <code>ConnectionHandler</code>, which is run in its own Tokio task:</p>
<figure class="highlight rust"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">tokio::<span class="title function_ invoke__">spawn</span>(<span class="keyword">async</span> <span class="keyword">move</span> {</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">let</span> <span class="variable">Err</span>(err) = handler.<span class="title function_ invoke__">handle_connection</span>().<span class="keyword">await</span> {</span><br><span class="line">        error!(<span class="string">"Connection error: {:?}"</span>, err);</span><br><span class="line">    }</span><br><span class="line">    <span class="title function_ invoke__">drop</span>(permit);</span><br><span class="line">});</span><br></pre></td></tr></tbody></table></figure>
</li>
<li><p><strong>All requests from all connections are funneled into a shared channel</strong>:<br>Each request, regardless of which connection it comes from, is sent into a central channel (<code>async_channel::Sender&lt;RequestTask&gt;</code>).</p>
</li>
<li><p><strong>A fixed-size pool of worker tasks processes all requests</strong>:<br>Instead of spawning a new task per request, a configurable number of worker tasks are started at server boot. These workers continuously pull requests from the channel and process them:</p>
<figure class="highlight rust"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> <span class="variable">i</span> <span class="keyword">in</span> <span class="number">0</span>..num_channels {</span><br><span class="line">    <span class="keyword">let</span> <span class="variable">rx</span>: async_channel::Receiver&lt;RequestTask&gt; = request_rx.<span class="title function_ invoke__">clone</span>();</span><br><span class="line">    <span class="keyword">let</span> <span class="variable">replica_manager</span> = replica_manager.<span class="title function_ invoke__">clone</span>();</span><br><span class="line">    <span class="keyword">let</span> <span class="variable">group_coordinator</span> = group_coordinator.<span class="title function_ invoke__">clone</span>();</span><br><span class="line">    <span class="keyword">let</span> <span class="variable">handle</span> = tokio::<span class="title function_ invoke__">spawn</span>(<span class="keyword">async</span> <span class="keyword">move</span> {</span><br><span class="line">        debug!(<span class="string">"request handler worker {} started"</span>, i);</span><br><span class="line">        <span class="keyword">while</span> <span class="keyword">let</span> <span class="variable">Ok</span>(request) = rx.<span class="title function_ invoke__">recv</span>().<span class="keyword">await</span> {</span><br><span class="line">            <span class="title function_ invoke__">process_request</span>(request, replica_manager.<span class="title function_ invoke__">clone</span>(), group_coordinator.<span class="title function_ invoke__">clone</span>()).<span class="keyword">await</span>;</span><br><span class="line">        }</span><br><span class="line">        debug!(<span class="string">"request handler worker {} exited"</span>, i);</span><br><span class="line">    });</span><br><span class="line">    workers.<span class="title function_ invoke__">insert</span>(i, handle);</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>

<p>This thread-pool-like model ensures that system resources are used efficiently, and the number of concurrent tasks is controlled.</p>
</li>
</ul>
<h4 id="Automatic-Worker-Recovery"><a href="#Automatic-Worker-Recovery" class="headerlink" title="Automatic Worker Recovery"></a>Automatic Worker Recovery</h4><p>To further enhance robustness, I implemented a monitoring mechanism:<br>A dedicated monitor task periodically checks the health of each worker. If a worker panics or exits unexpectedly, the monitor automatically spawns a replacement to maintain the desired pool size:</p>
<figure class="highlight rust"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> join_error.<span class="title function_ invoke__">is_panic</span>() {</span><br><span class="line">    <span class="comment">// ... log error ...</span></span><br><span class="line">    <span class="comment">// re-generate a new task</span></span><br><span class="line">    <span class="keyword">let</span> <span class="variable">rx</span> = request_rx.<span class="title function_ invoke__">clone</span>();</span><br><span class="line">    <span class="keyword">let</span> <span class="variable">replica_manager</span> = replica_manager.<span class="title function_ invoke__">clone</span>();</span><br><span class="line">    <span class="keyword">let</span> <span class="variable">group_coordinator</span> = group_coordinator.<span class="title function_ invoke__">clone</span>();</span><br><span class="line">    <span class="keyword">let</span> <span class="variable">new_worker</span> = tokio::<span class="title function_ invoke__">spawn</span>(<span class="keyword">async</span> <span class="keyword">move</span> {</span><br><span class="line">        <span class="keyword">while</span> <span class="keyword">let</span> <span class="variable">Ok</span>(request) = rx.<span class="title function_ invoke__">recv</span>().<span class="keyword">await</span> {</span><br><span class="line">            <span class="title function_ invoke__">process_request</span>(request, replica_manager.<span class="title function_ invoke__">clone</span>(), group_coordinator.<span class="title function_ invoke__">clone</span>()).<span class="keyword">await</span>;</span><br><span class="line">        }</span><br><span class="line">    });</span><br><span class="line">    workers.<span class="title function_ invoke__">insert</span>(id, new_worker);</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>

<h4 id="Benefits"><a href="#Benefits" class="headerlink" title="Benefits"></a>Benefits</h4><ul>
<li><strong>Resource Efficiency</strong>: By limiting the number of worker tasks, the server avoids the overhead of thousands of concurrent tasks under heavy load.</li>
<li><strong>Predictable Performance</strong>: The thread-pool model provides more consistent latency and throughput.</li>
<li><strong>Fault Tolerance</strong>: The monitor ensures that if a worker fails, it is quickly replaced, maintaining system stability.</li>
<li><strong>Scalability</strong>: The number of worker tasks can be tuned according to system capabilities and expected load.</li>
</ul>
<h4 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h4><p>This design pattern—dedicating a task per connection, centralizing request processing, and using a fixed-size worker pool—strikes a balance between concurrency and resource control. It’s a practical approach for building scalable, robust, and efficient async servers in Rust with Tokio.</p>
<p>Such requirements frequently arise across various scenarios. To address this, I developed a utility class that facilitates implementing such designs with ease. For detailed reference, please consult the <code>multiple_channel_worker_pool</code> and <code>single_channel_worker_pool</code> modules within the Utils package of <a target="_blank" rel="noopener" href="https://github.com/jonefeewang/stonemq">StoneMQ</a>.</p>
<h2 id="3-Design-Experience-Favoring-Lock-Free-Architectures-and-Minimizing-Use-of-Tokio-Async-Locks"><a href="#3-Design-Experience-Favoring-Lock-Free-Architectures-and-Minimizing-Use-of-Tokio-Async-Locks" class="headerlink" title="3. Design Experience: Favoring Lock-Free Architectures and Minimizing Use of Tokio Async Locks"></a>3. Design Experience: Favoring Lock-Free Architectures and Minimizing Use of Tokio Async Locks</h2><p>When architecting high-performance async systems in Rust, I strongly prefer designs that avoid locks—especially asynchronous locks—whenever possible. This principle is rooted in both performance considerations and code safety.</p>
<h4 id="Why-Avoid-Locks-Especially-Async-Locks"><a href="#Why-Avoid-Locks-Especially-Async-Locks" class="headerlink" title="Why Avoid Locks, Especially Async Locks?"></a>Why Avoid Locks, Especially Async Locks?</h4><ul>
<li><strong>Contention and Complexity</strong>: Traditional locks (<code>Mutex</code>, <code>RwLock</code>) can become contention points and introduce complexity, especially in concurrent environments.</li>
<li><strong>Async Lock Performance</strong>: Tokio’s asynchronous locks (<code>tokio::sync::Mutex</code>, <code>tokio::sync::RwLock</code>) are significantly slower than their synchronous counterparts. They are designed for correctness in async contexts, but their performance overhead can be substantial, especially under high contention or frequent lock/unlock cycles.</li>
<li><strong>Deadlock Risk</strong>: Holding any lock across <code>.await</code> points is risky and can easily lead to deadlocks or subtle bugs.</li>
</ul>
<h4 id="My-Approach-Channels-Message-Passing-and-Task-Ownership"><a href="#My-Approach-Channels-Message-Passing-and-Task-Ownership" class="headerlink" title="My Approach: Channels, Message Passing, and Task Ownership"></a>My Approach: Channels, Message Passing, and Task Ownership</h4><ul>
<li><strong>Single-threaded Task Ownership</strong>: Each async task owns its state, eliminating the need for locks in most cases.</li>
<li><strong>Channel-based Communication</strong>: Tasks interact via channels, passing messages instead of sharing mutable state.</li>
<li><strong>Minimal, Synchronous Locking</strong>: When shared mutable state is unavoidable, I use fine-grained, synchronous locks (<code>std::sync::Mutex</code>/<code>RwLock</code>), and always ensure locks are held for the shortest possible time and never across <code>.await</code>.</li>
<li><strong>Rare Use of Tokio Async Locks</strong>: I avoid Tokio’s async locks unless absolutely necessary. In almost all cases, the architecture is designed so that async locks are not required.</li>
</ul>
<h4 id="Example-Journal-Log-Write-Path"><a href="#Example-Journal-Log-Write-Path" class="headerlink" title="Example: Journal Log Write Path"></a>Example: Journal Log Write Path</h4><p>In the journal log write implementation, all critical sections that require locking are handled synchronously and released before any async operation:</p>
<figure class="highlight rust"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">{</span><br><span class="line">    <span class="comment">// Synchronously update active segment index</span></span><br><span class="line">    <span class="keyword">let</span> <span class="keyword">mut </span><span class="variable">active_seg_index</span> = <span class="keyword">self</span>.active_segment_index.<span class="title function_ invoke__">write</span>();</span><br><span class="line">    <span class="keyword">let</span> <span class="variable">old_segment_index</span> = std::mem::<span class="title function_ invoke__">replace</span>(&amp;<span class="keyword">mut</span> *active_seg_index, new_seg);</span><br><span class="line">    <span class="keyword">self</span>.active_segment_base_offset</span><br><span class="line">        .<span class="title function_ invoke__">store</span>(new_base_offset, Ordering::Release);</span><br><span class="line">    <span class="comment">// ... update other state ...</span></span><br><span class="line">} <span class="comment">// lock released here</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Async operations follow, with no lock held</span></span><br><span class="line"><span class="title function_ invoke__">get_journal_segment_writer</span>()</span><br><span class="line">    .<span class="title function_ invoke__">append_journal</span>(journal_log_write_op)</span><br><span class="line">    .<span class="keyword">await</span>?;</span><br></pre></td></tr></tbody></table></figure>

<h4 id="Benefits-1"><a href="#Benefits-1" class="headerlink" title="Benefits"></a>Benefits</h4><ul>
<li><strong>Performance</strong>: Avoiding Tokio’s async locks removes a major source of latency and contention in async Rust applications.</li>
<li><strong>Safety</strong>: By never holding locks across <code>.await</code>, I eliminate a whole class of async deadlocks.</li>
<li><strong>Simplicity</strong>: The code is easier to reason about, as ownership and mutability are clear and explicit.</li>
</ul>
<h4 id="Conclusion-1"><a href="#Conclusion-1" class="headerlink" title="Conclusion"></a>Conclusion</h4><p>By leveraging message passing, task-local state, and only minimal, synchronous locking, I achieve both high efficiency and safety in async Rust systems. Avoiding Tokio’s async locks is a deliberate choice, as their performance overhead is rarely justified except in very specific scenarios. This approach leads to scalable, maintainable, and robust code.</p>
<h2 id="4-Design-Experience-Judicious-Use-of-Unsafe-Code-for-Performance-Critical-Paths"><a href="#4-Design-Experience-Judicious-Use-of-Unsafe-Code-for-Performance-Critical-Paths" class="headerlink" title="4. Design Experience: Judicious Use of Unsafe Code for Performance-Critical Paths"></a>4. Design Experience: Judicious Use of Unsafe Code for Performance-Critical Paths</h2><p>In Rust, safety is a core language feature, but there are scenarios—especially in performance-critical systems programming—where the cost of absolute safety can be significant. In such cases, I carefully consider the use of <code>unsafe</code> code to achieve the necessary performance, while still maintaining overall correctness and encapsulation.</p>
<h4 id="Why-Use-Unsafe-Code"><a href="#Why-Use-Unsafe-Code" class="headerlink" title="Why Use Unsafe Code?"></a>Why Use Unsafe Code?</h4><ul>
<li><strong>Performance</strong>: Some operations, such as memory-mapped file access or direct byte manipulation, can be much faster when performed without the overhead of Rust’s safety checks.</li>
<li><strong>System-level Access</strong>: Certain low-level APIs (e.g., memory mapping, FFI, or direct buffer manipulation) require <code>unsafe</code> blocks to interact with OS or hardware resources.</li>
</ul>
<h4 id="My-Approach-Isolate-and-Audit-Unsafe-Code"><a href="#My-Approach-Isolate-and-Audit-Unsafe-Code" class="headerlink" title="My Approach: Isolate and Audit Unsafe Code"></a>My Approach: Isolate and Audit Unsafe Code</h4><ul>
<li><strong>Encapsulation</strong>: Unsafe code is always encapsulated within well-tested, minimal, and clearly documented functions or modules.</li>
<li><strong>Justification</strong>: Unsafe is only used when profiling or design analysis shows that safe alternatives are a bottleneck.</li>
<li><strong>Testing</strong>: I ensure that all unsafe code is covered by thorough tests and code reviews.</li>
</ul>
<h4 id="Example-Memory-Mapped-Index-Files"><a href="#Example-Memory-Mapped-Index-Files" class="headerlink" title="Example: Memory-Mapped Index Files"></a>Example: Memory-Mapped Index Files</h4><p>In the <code>index_file.rs</code> module, I use the <code>memmap2</code> crate to memory-map index files for fast, zero-copy access. Creating a memory map inherently requires <code>unsafe</code> code, as shown here:</p>
<figure class="highlight rust"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">let</span> <span class="variable">mmap</span> = <span class="keyword">unsafe</span> { MmapOptions::<span class="title function_ invoke__">new</span>().<span class="title function_ invoke__">map</span>(&amp;file)? };</span><br></pre></td></tr></tbody></table></figure>

<p>This allows the index file to be accessed as a byte slice, enabling efficient binary search and direct manipulation without extra copying or allocation. Similarly, for writable index files:</p>
<figure class="highlight rust"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">let</span> <span class="variable">mmap</span> = <span class="keyword">unsafe</span> { MmapOptions::<span class="title function_ invoke__">new</span>().<span class="title function_ invoke__">map_mut</span>(&amp;file)? };</span><br></pre></td></tr></tbody></table></figure>

<p>By isolating the <code>unsafe</code> block to just the memory mapping operation, the rest of the code can remain safe and idiomatic Rust.</p>
<h4 id="Benefits-2"><a href="#Benefits-2" class="headerlink" title="Benefits"></a>Benefits</h4><ul>
<li><strong>Maximum Performance</strong>: Direct memory access and zero-copy operations are critical for high-throughput log/index management.</li>
<li><strong>Controlled Risk</strong>: By limiting the scope of <code>unsafe</code>, I minimize the risk of undefined behavior.</li>
<li><strong>Maintainability</strong>: Encapsulated unsafe code is easier to audit and reason about.</li>
</ul>
<h4 id="Conclusion-2"><a href="#Conclusion-2" class="headerlink" title="Conclusion"></a>Conclusion</h4><p>While Rust’s safety guarantees are invaluable, there are times when performance requirements justify the careful use of <code>unsafe</code> code. By isolating and rigorously testing these sections, I can achieve both the speed and reliability needed for system-level components like log and index management.</p>
<hr>
<h2 id="5-Design-Experience-Separating-Mutable-and-Immutable-Data-to-Optimize-Lock-Granularity"><a href="#5-Design-Experience-Separating-Mutable-and-Immutable-Data-to-Optimize-Lock-Granularity" class="headerlink" title="5. Design Experience: Separating Mutable and Immutable Data to Optimize Lock Granularity"></a>5. Design Experience: Separating Mutable and Immutable Data to Optimize Lock Granularity</h2><p>A key architectural principle I follow in high-performance systems is to separate mutable and immutable data, and to use the finest possible lock granularity. This approach minimizes contention, improves concurrency, and makes the system easier to reason about.</p>
<h4 id="Why-Separate-Mutable-and-Immutable-Data"><a href="#Why-Separate-Mutable-and-Immutable-Data" class="headerlink" title="Why Separate Mutable and Immutable Data?"></a>Why Separate Mutable and Immutable Data?</h4><ul>
<li><strong>Reduced Contention</strong>: By isolating mutable state, only the truly changing parts of the system require synchronization, while immutable data can be freely shared and accessed without locks.</li>
<li><strong>Optimized Locking</strong>: Fine-grained locks (e.g., per-segment or per-structure) allow multiple operations to proceed in parallel, rather than serializing all access through a single global lock.</li>
<li><strong>Clearer Ownership</strong>: The distinction between mutable and immutable data clarifies which parts of the code can safely share data and which require careful coordination.</li>
</ul>
<h4 id="My-Approach-Example-from-Journal-Log-Management"><a href="#My-Approach-Example-from-Journal-Log-Management" class="headerlink" title="My Approach: Example from Journal Log Management"></a>My Approach: Example from Journal Log Management</h4><p>In the journal log implementation, I apply this principle by:</p>
<ul>
<li><strong>Using separate structures for mutable and immutable data</strong>:  <ul>
<li>The <code>active_segment_index</code> (the currently writable segment) is protected by a <code>RwLock</code>, allowing multiple readers or a single writer.</li>
<li>The set of segment base offsets (<code>segments_order</code>) is also protected by a <code>RwLock</code>, but is only modified when segments are added or removed.</li>
<li>All read-only segments are stored as <code>Arc&lt;ReadOnlySegmentIndex&gt;</code> in a concurrent <code>DashMap</code>, allowing lock-free concurrent reads.</li>
</ul>
</li>
</ul>
<figure class="highlight rust"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#[derive(Debug)]</span></span><br><span class="line"><span class="keyword">pub</span> <span class="keyword">struct</span> <span class="title class_">JournalLog</span> {</span><br><span class="line">    <span class="comment">// Ordered set of segment base offsets (non-active segments)</span></span><br><span class="line">    segments_order: RwLock&lt;BTreeSet&lt;<span class="type">i64</span>&gt;&gt;,</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Map of base offsets to read-only segments</span></span><br><span class="line">    segment_index: DashMap&lt;<span class="type">i64</span>, Arc&lt;ReadOnlySegmentIndex&gt;&gt;,</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Currently active segment</span></span><br><span class="line">    active_segment_index: RwLock&lt;ActiveSegmentIndex&gt;,</span><br><span class="line"></span><br><span class="line">    <span class="comment">// ... other fields ...</span></span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>

<ul>
<li><p><strong>Read-only data is stored in <code>ReadOnlySegmentIndex</code></strong>:<br>Once a segment is no longer active, it is converted to a read-only structure and stored in an <code>Arc</code>, making it safe to share across threads without any locking.</p>
</li>
<li><p><strong>Lock scope is minimized</strong>:<br>For example, when rolling a segment, the lock on <code>active_segment_index</code> is only held for the duration of the swap, and not across any async or I/O operations.</p>
</li>
</ul>
<h4 id="Benefits-3"><a href="#Benefits-3" class="headerlink" title="Benefits"></a>Benefits</h4><ul>
<li><strong>High Concurrency</strong>: Multiple threads can read from different segments or query the segment index concurrently, without blocking each other.</li>
<li><strong>Minimal Locking Overhead</strong>: Only the small, mutable parts of the system are protected by locks, and those locks are held for the shortest possible time.</li>
<li><strong>Scalability</strong>: The system can efficiently handle many concurrent operations, such as reads, writes, and segment management.</li>
</ul>
<h4 id="Conclusion-3"><a href="#Conclusion-3" class="headerlink" title="Conclusion"></a>Conclusion</h4><p>By separating mutable and immutable data, and applying the smallest possible lock granularity, I achieve both high performance and maintainability. This pattern is especially effective in log-structured storage and similar systems, where most data is append-only or read-mostly, and only a small portion is actively mutated.</p>
<h2 id="6-Separate-Asynchronous-and-Synchronous-Data-Operations-to-Optimize-Lock-Usage"><a href="#6-Separate-Asynchronous-and-Synchronous-Data-Operations-to-Optimize-Lock-Usage" class="headerlink" title="6. Separate Asynchronous and Synchronous Data Operations to Optimize Lock Usage"></a>6. Separate Asynchronous and Synchronous Data Operations to Optimize Lock Usage</h2><p>The locking requirements of asynchronous and synchronous code differ fundamentally: holding a synchronous lock within asynchronous code can cause the entire Future dependency chain to block, negating the benefits of the asynchronous model. Conversely, asynchronous locks incur higher overhead and employ more complex mechanisms.</p>
<p>The best practice is to <strong>segregate the data involved in asynchronous operations from that used in synchronous operations</strong>, ensuring that locks protecting synchronous data do not span asynchronous contexts. This allows synchronous portions to safely utilize efficient synchronous locks (such as <code>std::sync::Mutex</code>), while the asynchronous parts avoid blocking or contention.</p>
<p>By isolating data domains and restructuring access patterns accordingly, you can significantly reduce lock contention and blocking, thereby enhancing asynchronous execution efficiency.</p>
<h4 id="Example-StoneMQ-Journal-Log-Write-Path"><a href="#Example-StoneMQ-Journal-Log-Write-Path" class="headerlink" title="Example: StoneMQ Journal Log Write Path"></a>Example: <a href="">StoneMQ</a> Journal Log Write Path</h4><p>In my project, the journal log write implementation demonstrates this pattern clearly. Here’s a simplified version:</p>
<figure class="highlight rust"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Synchronously update the active segment index</span></span><br><span class="line">{</span><br><span class="line">    <span class="keyword">let</span> <span class="keyword">mut </span><span class="variable">active_seg_index</span> = <span class="keyword">self</span>.active_segment_index.<span class="title function_ invoke__">write</span>();</span><br><span class="line">    <span class="keyword">let</span> <span class="variable">old_segment_index</span> = std::mem::<span class="title function_ invoke__">replace</span>(&amp;<span class="keyword">mut</span> *active_seg_index, new_seg);</span><br><span class="line">    <span class="comment">// ... update other state ...</span></span><br><span class="line">} <span class="comment">// lock released here</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Now perform async operations, with no lock held</span></span><br><span class="line"><span class="title function_ invoke__">get_journal_segment_writer</span>()</span><br><span class="line">    .<span class="title function_ invoke__">append_journal</span>(journal_log_write_op)</span><br><span class="line">    .<span class="keyword">await</span>?;</span><br></pre></td></tr></tbody></table></figure>

<p>Notice how the lock is acquired, the necessary mutation is performed, and then the lock is released <strong>before</strong> any async operation is awaited.</p>
<h4 id="Example-Group-Coordinator"><a href="#Example-Group-Coordinator" class="headerlink" title="Example: Group Coordinator"></a>Example: Group Coordinator</h4><p>In the group coordinator logic, the same principle is applied. Before calling any async function, the lock is explicitly dropped:</p>
<figure class="highlight rust"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="title function_ invoke__">drop</span>(locked_group);</span><br><span class="line"><span class="keyword">self</span>.<span class="title function_ invoke__">maybe_prepare_rebalance</span>(group).<span class="keyword">await</span>;</span><br></pre></td></tr></tbody></table></figure>

<p>This ensures that no lock is ever held across an <code>.await</code>, preventing deadlocks and maximizing concurrency.</p>
<h3 id="Benefits-4"><a href="#Benefits-4" class="headerlink" title="Benefits"></a>Benefits</h3><ul>
<li><strong>Performance</strong>: Synchronous locks are much faster than async locks, and can be used safely when not crossing async boundaries.</li>
<li><strong>Safety</strong>: Avoids deadlocks and subtle bugs that can arise from holding locks across <code>.await</code>.</li>
<li><strong>Clarity</strong>: The code is easier to reason about, as the boundaries between sync and async operations are explicit.</li>
</ul>
<h3 id="Conclusion-4"><a href="#Conclusion-4" class="headerlink" title="Conclusion"></a>Conclusion</h3><p>By separating async and sync data, and ensuring that locks are only used for sync data and never held across <code>.await</code>, you can write high-performance, safe, and maintainable async Rust code. This pattern is a cornerstone of robust async system design, and has paid off time and again in my own projects.</p>
<h2 id="7-Employ-Static-Dispatch-in-Performance-Critical-Paths-Whenever-Possible"><a href="#7-Employ-Static-Dispatch-in-Performance-Critical-Paths-Whenever-Possible" class="headerlink" title="7. Employ Static Dispatch in Performance-Critical Paths Whenever Possible"></a>7. Employ Static Dispatch in Performance-Critical Paths Whenever Possible</h2><h3 id="Design-Experience-Using-Enums-for-Protocol-Type-Polymorphism-in-Performance-Critical-Code"><a href="#Design-Experience-Using-Enums-for-Protocol-Type-Polymorphism-in-Performance-Critical-Code" class="headerlink" title="Design Experience: Using Enums for Protocol Type Polymorphism in Performance-Critical Code"></a>Design Experience: Using Enums for Protocol Type Polymorphism in Performance-Critical Code</h3><p>When building the protocol layer for <a target="_blank" rel="noopener" href="https://github.com/jonefeewang/stonemq">StoneMQ</a>, I faced a key design decision: how to represent and handle the various data types that appear in protocol messages. The two main options were using Rust’s trait objects for dynamic polymorphism, or using an enum to statically represent all possible protocol types.</p>
<h4 id="Why-Not-Trait-Objects"><a href="#Why-Not-Trait-Objects" class="headerlink" title="Why Not Trait Objects?"></a>Why Not Trait Objects?</h4><p>Trait objects (<code>dyn Trait</code>) are a common way to achieve polymorphism in Rust. They allow different types to be handled through a common interface, with the actual method implementations determined at runtime. This approach is flexible and can make code easier to extend. However, it comes with some downsides:</p>
<ul>
<li><strong>Dynamic dispatch</strong>: Every method call on a trait object involves a runtime lookup, which adds overhead.</li>
<li><strong>Heap allocation</strong>: Trait objects often require heap allocation, especially when used in collections.</li>
<li><strong>Performance impact</strong>: In a performance-critical layer like a network protocol, even small inefficiencies can add up.</li>
</ul>
<h4 id="The-Enum-Approach"><a href="#The-Enum-Approach" class="headerlink" title="The Enum Approach"></a>The Enum Approach</h4><p>To avoid these costs, I chose to use an enum—<code>ProtocolType</code>—to represent all protocol data types. Each variant of the enum corresponds to a specific protocol type, such as <code>I8</code>, <code>I16</code>, <code>PString</code>, <code>Array</code>, etc. Here’s a simplified excerpt from the actual code:</p>
<figure class="highlight rust"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">pub</span> <span class="keyword">enum</span> <span class="title class_">ProtocolType</span> {</span><br><span class="line">    <span class="title function_ invoke__">Bool</span>(Bool),</span><br><span class="line">    <span class="title function_ invoke__">I8</span>(I8),</span><br><span class="line">    <span class="title function_ invoke__">I16</span>(I16),</span><br><span class="line">    <span class="title function_ invoke__">I32</span>(I32),</span><br><span class="line">    <span class="title function_ invoke__">U32</span>(U32),</span><br><span class="line">    <span class="title function_ invoke__">I64</span>(I64),</span><br><span class="line">    <span class="title function_ invoke__">PString</span>(PString),</span><br><span class="line">    <span class="title function_ invoke__">NPString</span>(NPString),</span><br><span class="line">    <span class="title function_ invoke__">PBytes</span>(PBytes),</span><br><span class="line">    <span class="title function_ invoke__">NPBytes</span>(NPBytes),</span><br><span class="line">    <span class="title function_ invoke__">PVarInt</span>(PVarInt),</span><br><span class="line">    <span class="title function_ invoke__">PVarLong</span>(PVarLong),</span><br><span class="line">    <span class="title function_ invoke__">Array</span>(ArrayType),</span><br><span class="line">    <span class="title function_ invoke__">Records</span>(MemoryRecords),</span><br><span class="line">    <span class="title function_ invoke__">Schema</span>(Arc&lt;Schema&gt;),</span><br><span class="line">    <span class="title function_ invoke__">ValueSet</span>(ValueSet),</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>

<p>This design allows me to match on the enum and handle each type explicitly, with all dispatch resolved at compile time. For example, when calculating the wire format size of a protocol value, I can use a match statement:</p>
<figure class="highlight rust"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">pub</span> <span class="keyword">fn</span> <span class="title function_">size</span>(&amp;<span class="keyword">self</span>) <span class="punctuation">-&gt;</span> <span class="type">i32</span> {</span><br><span class="line">    <span class="keyword">match</span> <span class="keyword">self</span> {</span><br><span class="line">        ProtocolType::<span class="title function_ invoke__">Bool</span>(<span class="type">bool</span>) =&gt; <span class="type">bool</span>.<span class="title function_ invoke__">wire_format_size</span>() <span class="keyword">as</span> <span class="type">i32</span>,</span><br><span class="line">        ProtocolType::<span class="title function_ invoke__">I8</span>(<span class="type">i8</span>) =&gt; <span class="type">i8</span>.<span class="title function_ invoke__">wire_format_size</span>() <span class="keyword">as</span> <span class="type">i32</span>,</span><br><span class="line">        <span class="comment">// ... other types ...</span></span><br><span class="line">        ProtocolType::<span class="title function_ invoke__">Array</span>(array) =&gt; array.<span class="title function_ invoke__">size</span>() <span class="keyword">as</span> <span class="type">i32</span>,</span><br><span class="line">        <span class="comment">// ...</span></span><br><span class="line">        ProtocolType::<span class="title function_ invoke__">Schema</span>(_) =&gt; {</span><br><span class="line">            <span class="built_in">panic!</span>(<span class="string">"Unexpected calculation of schema size"</span>);</span><br><span class="line">        }</span><br><span class="line">        ProtocolType::<span class="title function_ invoke__">ValueSet</span>(valueset) =&gt; valueset.<span class="title function_ invoke__">size</span>() <span class="keyword">as</span> <span class="type">i32</span>,</span><br><span class="line">    }</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>

<h4 id="Benefits-in-Practice"><a href="#Benefits-in-Practice" class="headerlink" title="Benefits in Practice"></a>Benefits in Practice</h4><ul>
<li><strong>Performance</strong>: All type dispatch is handled at compile time, with no runtime overhead from dynamic dispatch.</li>
<li><strong>Type Safety</strong>: The compiler ensures that all protocol types are handled, reducing the risk of runtime errors.</li>
<li><strong>Extensibility</strong>: Adding a new protocol type is as simple as adding a new enum variant and updating the relevant match arms.</li>
</ul>
<h4 id="Example-Array-Construction"><a href="#Example-Array-Construction" class="headerlink" title="Example: Array Construction"></a>Example: Array Construction</h4><p>I also used macros and generic functions to make it easy to construct arrays of protocol types:</p>
<figure class="highlight rust"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">macro_rules!</span> array_of {</span><br><span class="line">    ($func_name:ident, $<span class="keyword">type</span>:ty) =&gt; {</span><br><span class="line">        <span class="keyword">pub</span> <span class="keyword">fn</span> $<span class="title function_ invoke__">func_name</span>(value: $<span class="keyword">type</span>) <span class="punctuation">-&gt;</span> ProtocolType {</span><br><span class="line">            ProtocolType::<span class="title function_ invoke__">Array</span>(ArrayType {</span><br><span class="line">                can_be_empty: <span class="literal">false</span>,</span><br><span class="line">                p_type: Arc::<span class="title function_ invoke__">new</span>(value.<span class="title function_ invoke__">into</span>()),</span><br><span class="line">                values: <span class="literal">None</span>,</span><br><span class="line">            })</span><br><span class="line">        }</span><br><span class="line">    };</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="comment">// Usage:</span></span><br><span class="line">array_of!(array_of_i32_type, <span class="type">i32</span>);</span><br><span class="line">array_of!(array_of_string_type, <span class="type">String</span>);</span><br></pre></td></tr></tbody></table></figure>

<h4 id="Conclusion-5"><a href="#Conclusion-5" class="headerlink" title="Conclusion"></a>Conclusion</h4><p>By using an enum to represent protocol types, I achieved efficient, type-safe, and maintainable code for the protocol layer. This approach is especially valuable in systems where performance is critical and the set of types is known and relatively stable.</p>
<h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>Achieving high-performance asynchronous Rust projects transcends mere usage of the async/await syntax; it fundamentally relies on a deep understanding of the underlying task scheduling, lock optimization, and architecture design principles. From eschewing gratuitous async functions to judiciously managing lock granularity; from minimizing the number of spawned tasks to adopting a message-passing architecture; from cautious use of unsafe code to segregating asynchronous and synchronous data; and finally, to maximizing the use of static dispatch—each practice serves to bolster both the efficiency and robustness of your application.</p>
<p>I hope this summary offers valuable insights and assistance to those engaged in Rust asynchronous development. Contributions and discussions are warmly welcomed, as we collectively strive to elevate the practical maturity of the Rust async ecosystem!</p>
</div><div class="post-copyright"><script type="text/javascript" src="/js/copyright.js?v=1.0.0" successtext="Copy Successed!"></script><link rel="stylesheet" type="text/css" href="/css/copyright.css?v=1.0.0"><p><span>Title: </span>Rewriting Kafka in Rust Async: Insights and Lessons Learned in Rust</p><p><span>Author: </span>Rex Wang rexwang735@gmail.com</p><p><span>Date: </span>2025-06-18</p><p><span>Last Update: </span>2025-06-18</p><p><span>Blog Link: </span><a href="/2025/06/18/Rewriting-Kafka-in-Rust-Async-Insights-and-Lessons-Learned/">https://wangjunfei.com/2025/06/18/Rewriting-Kafka-in-Rust-Async-Insights-and-Lessons-Learned/</a><span class="copy-path"><i class="fa fa-clipboard" data-clipboard-text="https://wangjunfei.com/2025/06/18/Rewriting-Kafka-in-Rust-Async-Insights-and-Lessons-Learned/"></i></span></p><p><span>Copyright Declaration: </span>Plz credit the original source when reposting</p></div><br><script type="text/javascript" src="/js/share.js?v=1.0.0" async></script><a class="article-share-link" data-url="https://wangjunfei.com/2025/06/18/Rewriting-Kafka-in-Rust-Async-Insights-and-Lessons-Learned/" data-id="cmc1cedqp000jdl82e6ax1oll" data-qrcode="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAPYAAAD2CAAAAADAeSUUAAADJ0lEQVR42u3aSXLbQBAEQP7/0/JVETYZVd2QRQwTJwaxTc6l0cvjER9f346//392ffL/6yu/n23fdcGBjY2NfRP218sjX0T7hGd3zdacW7CxsbFPZb8ODK8f/fr6WbCJlr55LzY2NvYHs9twlWxQ8q7ZvdjY2NjYeZBIlpUUmJLN+rW4jY2Njf3G7E3IuTYUJXf911oaNjY29tuz8xLP+//+kf42NjY29huzv8ojH6zJl7gf4qkV2NjY2Aex8wCQD9zMyk+bMdDh+rGxsbEPYm9CUT5MORvu2WxitK3Y2NjYR7CTxCMpsl87fD8bACpc2NjY2EewN23XBNluWdvWHQZabGxs7IPY+Yf7tQM6m2LQbCX15Ck2Njb2Tdj7UcsNYzbumRehnq4NGxsb+zh2ftusPZAsepO65MHyaVEJGxsb++bszTjOvmEwKw/Nyl7RaWxsbOxbsZNUIS85zRY9S2/ykaAoWcLGxsa+LbsdpmmngTat4jZ1Kc5iY2NjH8reNANmpai8IbEZDPrHc7CxsbGPY7eBpMW0Z5MEY1VgwsbGxj6OvZnczF/Zhpk8lNYrwcbGxj6InQek2RjNrH2bJDxtY+Axy5OwsbGx3549+/TfF4xmzYCkmVEPX2JjY2PfnJ2Hn801s9bvLOGp29XY2NjYN2fnH/H7pc8G3Ns0Jgmi2NjY2Oex26MtJ21+twlG3RjAxsbGPo79iI/ZFuTIa2v70aQSNjY29m3Zs8RjdjZPYPJxn3a7sbGxsT+N3TZWN0FoltLUKRM2Njb2QezZo9sRmVlhKH9CEnqj/jY2Njb2bdn5UE6+HfnS88CzuRIbGxv7c9izoDUb35mNeM4aCdjY2Nifw26DxGabZk2Iy9rS2NjY2EewZ2OXPxGWZsOXbVKEjY2NfR67DQDXJif7scu2zVzjsbGxsW/C3hT0r9rXBPOanWxK0UnAxsbGviF7M/I4G7VswW3AiwIYNjY29key28DTBrlZMagd8cTGxsbGTkLL5uymFR09HxsbG/s4dt7czcNYUqLKy1sXB1RsbGzsg9ib8n1Sjm9D3SyAJU/AxsbGPo79B1uDAl1GHabcAAAAAElFTkSuQmCC">Aktie</a><div class="tags"><ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Kafka/" rel="tag">Kafka</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Message-Queue/" rel="tag">Message Queue</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Pulsar/" rel="tag">Pulsar</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Rust/" rel="tag">Rust</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/StoneMQ/" rel="tag">StoneMQ</a></li></ul></div><div class="post-nav"><a class="next" href="/2025/02/10/Announcing-Stonemq-A-high-performance-and-efficient-message-queue-developed-in-Rust/">Announcing Stonemq: A high-performance and efficient message queue developed in Rust</a></div><div class="nofancybox" id="vcomment"></div><script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script><script src="//unpkg.com/valine@latest/dist/Valine.min.js"></script><script>var notify = 'false' == 'true' ? true : false;
var verify = 'true' == 'true' ? true : false;
var GUEST_INFO = ['nick','mail','link'];
var guest_info = 'nick,mail,link'.split(',').filter(function(item){
  return GUEST_INFO.indexOf(item) > -1
});
guest_info = guest_info.length == 0 ? GUEST_INFO :guest_info;
window.valine = new Valine({
  el:'#vcomment',
  notify:notify,
  verify:verify,
  appId:'FNjCNRwQqC3sTPP04DfRFIk9-gzGzoHsz',
  appKey:'LzCq5t6a6RTkwXQVr8xLk0yj',
  serverURLs:'',
  placeholder:'Please provide constructive and respectful feedback.',
  avatar:'monsterid',
  guest_info:guest_info,
  pageSize:'10'
})
</script></div></div></div><div class="pure-u-1 pure-u-md-1-4"><div id="sidebar"><div class="widget"><form class="search-form" action="//www.baidu.com/baidu" method="get" accept-charset="utf-8" target="_blank"><input type="search" name="word" maxlength="20" placeholder="Search"/><input type="hidden" name="si" value="https://wangjunfei.com"/><input name="tn" type="hidden" value="bds"/><input name="cl" type="hidden" value="3"/><input name="ct" type="hidden" value="2097152"/><input name="s" type="hidden" value="on"/></form></div><div class="widget"><div class="author-info"><a class="info-avatar" href="/about/" title="Über"><img class="nofancybox" src="/img/avatar.png"/></a><p>Always be the best</p><a class="info-icon" href="mailto:rexwang735@gmail.com" title="Email" target="_blank" style="margin-inline:5px"> <i class="fa fa-envelope-square" style="margin-inline:5px"></i></a><a class="info-icon" href="https://github.com/jonefeewang" title="Github" target="_blank" style="margin-inline:5px"> <i class="fa fa-github-square" style="margin-inline:5px"></i></a><a class="info-icon" href="/atom.xml" title="RSS" target="_blank" style="margin-inline:5px"> <i class="fa fa-rss-square" style="margin-inline:5px"></i></a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> Kategorien</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%88%86%E5%B8%83%E5%BC%8F%E6%8A%80%E6%9C%AF/">分布式技术</a><span class="category-list-count">3</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%8A%A1%E8%99%9A-%E4%BA%A4%E6%B5%81%E6%B2%9F%E9%80%9A/">务虚-交流沟通</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%8A%A1%E8%99%9A-%E7%AE%A1%E7%90%86%E6%96%B9%E6%B3%95%E8%AE%BA/">务虚-管理方法论</a><span class="category-list-count">8</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/">消息队列</a><span class="category-list-count">9</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E7%A3%81%E7%9B%98%E8%AF%BB%E5%86%99%E4%BC%98%E5%8C%96/">磁盘读写优化</a><span class="category-list-count">1</span></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> Tags</i></div><div class="tagcloud"><a href="/tags/Apache-Pulsar/" style="font-size: 15px;">Apache Pulsar</a> <a href="/tags/Apache-Bookkeeper/" style="font-size: 15px;">Apache Bookkeeper</a> <a href="/tags/Apache-Kafka/" style="font-size: 15px;">Apache Kafka</a> <a href="/tags/Apache-pulsar-%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97-kafka-RocketMQ/" style="font-size: 15px;">Apache pulsar 消息队列 kafka RocketMQ</a> <a href="/tags/Message-Queue/" style="font-size: 15px;">Message Queue</a> <a href="/tags/Rust/" style="font-size: 15px;">Rust</a> <a href="/tags/Kafka/" style="font-size: 15px;">Kafka</a> <a href="/tags/Pulsar/" style="font-size: 15px;">Pulsar</a> <a href="/tags/StoneMQ/" style="font-size: 15px;">StoneMQ</a> <a href="/tags/kafka-raft-kraft-zookeeper/" style="font-size: 15px;">kafka raft kraft zookeeper</a> <a href="/tags/%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/" style="font-size: 15px;">消息队列</a> <a href="/tags/rocketmq/" style="font-size: 15px;">rocketmq</a> <a href="/tags/kafka/" style="font-size: 15px;">kafka</a> <a href="/tags/pulsar/" style="font-size: 15px;">pulsar</a> <a href="/tags/%E5%8A%A1%E8%99%9A-%E7%AE%A1%E7%90%86%E6%96%B9%E6%B3%95%E8%AE%BA/" style="font-size: 15px;">务虚-管理方法论</a> <a href="/tags/%E5%8D%8E%E4%B8%BA/" style="font-size: 15px;">华为</a> <a href="/tags/%E7%AE%A1%E7%90%86%E6%96%B9%E6%B3%95%E8%AE%BA/" style="font-size: 15px;">管理方法论</a> <a href="/tags/%E7%AE%A1%E7%90%86/" style="font-size: 15px;">管理</a> <a href="/tags/%E5%8A%A1%E8%99%9A-%E9%A2%86%E5%AF%BC%E6%A2%AF%E9%98%9F/" style="font-size: 15px;">务虚-领导梯队</a> <a href="/tags/%E5%8A%A1%E8%99%9A/" style="font-size: 15px;">务虚</a> <a href="/tags/%E5%8D%8F%E4%BD%9C/" style="font-size: 15px;">协作</a> <a href="/tags/%E8%BD%AF%E7%B4%A0%E8%B4%A8/" style="font-size: 15px;">软素质</a> <a href="/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E6%8A%80%E6%9C%AF/" style="font-size: 15px;">分布式技术</a> <a href="/tags/zookeeper/" style="font-size: 15px;">zookeeper</a> <a href="/tags/consensus-algorithm/" style="font-size: 15px;">consensus algorithm</a> <a href="/tags/zab/" style="font-size: 15px;">zab</a> <a href="/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E5%8D%8F%E8%AE%AE/" style="font-size: 15px;">分布式协议</a> <a href="/tags/%E4%B8%80%E8%87%B4%E6%80%A7%E7%AE%97%E6%B3%95/" style="font-size: 15px;">一致性算法</a> <a href="/tags/%E6%96%B9%E6%B3%95%E8%AE%BA-%E8%BD%AF%E7%B4%A0%E8%B4%A8%E6%8F%90%E5%8D%87/" style="font-size: 15px;">方法论 软素质提升</a> <a href="/tags/kafka-netty-%E6%97%B6%E9%97%B4%E8%BD%AE/" style="font-size: 15px;">kafka netty 时间轮</a> <a href="/tags/%E4%BA%91%E5%8E%9F%E7%94%9F/" style="font-size: 15px;">云原生</a> <a href="/tags/%E6%B5%81%E5%BC%8F%E8%AE%A1%E7%AE%97/" style="font-size: 15px;">流式计算</a> <a href="/tags/%E7%A3%81%E7%9B%98%E8%AF%BB%E5%86%99%E4%BC%98%E5%8C%96/" style="font-size: 15px;">磁盘读写优化</a> <a href="/tags/mysql/" style="font-size: 15px;">mysql</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> Letzte</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2025/06/18/Rewriting-Kafka-in-Rust-Async-Insights-and-Lessons-Learned/">Rewriting Kafka in Rust Async: Insights and Lessons Learned in Rust</a></li><li class="post-list-item"><a class="post-list-link" href="/2025/02/10/Announcing-Stonemq-A-high-performance-and-efficient-message-queue-developed-in-Rust/">Announcing Stonemq: A high-performance and efficient message queue developed in Rust</a></li><li class="post-list-item"><a class="post-list-link" href="/2023/12/25/Apache-Pulsar%E7%9A%84%E5%AD%98%E5%82%A8%E6%A8%A1%E5%9E%8B/">Apache Pulsar的存储模型</a></li><li class="post-list-item"><a class="post-list-link" href="/2023/09/15/Apache-pulsar%E5%AF%B9zookeeper%E7%9A%84%E4%BE%9D%E8%B5%96%E5%92%8C%E5%8E%BB%E9%99%A4%E5%88%86%E6%9E%90-%E4%BA%8C/">Apache pulsar对zookeeper的依赖和去除分析(二)</a></li><li class="post-list-item"><a class="post-list-link" href="/2023/06/15/Apache-pulsar%E5%AF%B9zookeeper%E7%9A%84%E4%BE%9D%E8%B5%96%E5%92%8C%E5%8E%BB%E9%99%A4%E5%88%86%E6%9E%90-%E4%B8%80/">Apache pulsar对zookeeper的依赖和去除分析(一)</a></li><li class="post-list-item"><a class="post-list-link" href="/2023/04/10/%E6%97%B6%E9%97%B4%E8%BD%AE%E7%AE%97%E6%B3%95%E5%88%86%E6%9E%90%E5%92%8C%E5%BA%94%E7%94%A8/">时间轮算法分析和应用</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/12/01/Kafka%E5%8E%BBZookeeper%E6%8F%AD%E7%A7%98/">Kafka去Zookeeper揭秘</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/09/15/PRFAQ%E7%BC%96%E5%86%99%E8%A7%84%E8%8C%83/">「转」PRFAQ编写规范</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/09/13/%E5%87%A0%E7%A7%8D%E5%B8%B8%E8%A7%81%E7%9A%84%E5%B7%A5%E4%BD%9C%E6%96%B9%E6%B3%95%E8%AE%BA%E6%B1%87%E9%9B%86/">「转」几种常见的工作方法论汇集</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/08/14/%E8%BD%AF%E7%B4%A0%E8%B4%A8%E8%9E%BA%E6%97%8B%E4%B8%8A%E5%8D%87/">软素质螺旋上升</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-external-link"> Blogroll</i></div><ul></ul><a href="http://www.example1.com/" title="site-name1" target="_blank">site-name1</a></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2025 <a href="/." rel="nofollow">Rex Wang.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a class="show" id="rocket" href="#top"></a><script type="text/javascript" src="/js/totop.js?v=1.0.0" async></script><script type="text/javascript" src="//lib.baomitu.com/fancybox/latest/jquery.fancybox.min.js"></script><script type="text/javascript" src="/js/fancybox.js?v=1.0.0"></script><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/fancybox/latest/jquery.fancybox.min.css"><script type="text/javascript" src="/js/copycode.js?v=1.0.0" successtext="Copy Successed!"></script><link rel="stylesheet" type="text/css" href="/css/copycode.css?v=1.0.0"><script type="text/javascript" src="/js/codeblock-resizer.js?v=1.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=1.0.0"></script></div></body></html>